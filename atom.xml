<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>muzi-8</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-08-19T14:55:51.245Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>muzi</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>文献DeepTracker:Visualizing the Training Process of Convolutional Neural Networks解读Ⅰ</title>
    <link href="http://yoursite.com/2018/08/17/%E6%96%87%E7%8C%AEDeepTrackerVisualizing%20the%20Training%20Process%20of%20Convolutional%20Neural%20Networks%E8%A7%A3%E8%AF%BB/"/>
    <id>http://yoursite.com/2018/08/17/文献DeepTrackerVisualizing the Training Process of Convolutional Neural Networks解读/</id>
    <published>2018-08-17T07:58:54.000Z</published>
    <updated>2018-08-19T14:55:51.245Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>这篇文章能够进入到我的视线，缘由也是在听屈老师介绍他们组的工作，这篇文章出自国内香港科技大学屈华民老师组的一篇大作，此文章收录在期刊<a href="https://tist.acm.org/#" target="_blank" rel="noopener">TIST</a>(ACM Transactions on Intelligent Systems and Technology),在自己博士课题方向的刚开始，接触此领域关注的第一篇文章是CNNvis，出自清华大学刘世霞老师组。</p><h2 id="摘要-动机"><a href="#摘要-动机" class="headerlink" title="摘要/动机"></a>摘要/动机</h2><p>深度卷积网络已经在各个领域取得巨大成功。为了加速训练过程，并且减少试错次数，专家们首先需要理解在训练过程中发生的事情和理解卷积网络的行为，虽然目前有Tensorboard训练可视化平台，但是这个平台只能提供少量的信息，比如训练集/验证集的loss值等。为了跨越此鸿沟，作者提出了一个可视分析系统，DeepTracker，对训练过程的动态信息进行探索和发现隐藏在大量训练日志的异常模式。</p><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>可视隐藏在动态训练过程中的信息对理解CNN网络至关重要（例如：损失值、正确率的变化；权重、梯度、激活值随时间的变化等），不幸地是CNNs网络通常不仅包含了许多相互制约与非线性部分，并且近来网络也变得越宽越深，这些都给专家在推断CNN训练行为带来了很多困难。</p><p>有很多研究工作关注于CNN网络学习到的特征，或者网络训练过程的某些代表性快照(snapshots)。不过，很少有研究关注可视训练的整个动态过程（正是本文献所要进行的工作），目前存在的一些工具，比如：TensorBoard、Nvidia Digist、Deeplearning4j<strong>不能够解决工业级别（industry-level）的训练</strong>（在非常大的训练集Imagenet上训练大型网络），作者也提到现有的这些工作不能说明以下一些问题：</p><ul><li><p>随着迭代次数的增加模型在每一个图像类别的性能变化如何？</p></li><li><p>参数的变化是如何影响每一个类别的分类结果？</p></li><li><p>如此多的层数和如此多的图像类别，哪些才更值得关注？</p></li></ul><p>有了上述这些顾虑，亟需一种<strong>可扩展（scalable）</strong>的可视化解决方案。那么会面临哪些挑战呢？</p><ul><li><p>第一大挑战：处理大量（large-scale）的训练日志数据</p><ul><li>百万参数、上万张验证集、百万迭代次数，然而参数与分类结果需要几个迭代次数就得记录一次</li></ul></li><li><p>第二大挑战：日志信息是异构的</p><ul><li>结构信息（网络结构）、数值信息（神经元权重）、图像信息（验证数据集）、标定数据（分类结果）</li></ul></li></ul><p>面对这两大挑战，作者提出了如下解决方案：</p><p>后端算法：</p><ul><li><p>采用下采样方法存储、预处理、组织数据</p></li><li><p>高效的索引机制保证实时（real-time）交互 </p></li><li><p><strong>面向应用的异常检测方法</strong></p></li><li><p>集成<strong>过滤</strong>、<strong>整合</strong>方法 </p></li></ul><p>前端可视化：</p><ul><li><p>立方体（cube-style）可视化形式揭示神经元权重、验证集数据、训练的迭代次数。从不同的角度进行slice、dice数据。<br>文章的主要贡献点总结如下：</p></li><li><p>系统化分析问题与需求</p></li><li><p>系统化地后端算法与前端可视化方法</p></li><li><p>一些新颖的可视化与交互技巧：hierarchical small multiples、grid-based correlation view、cube-style visualization.</p><h2 id="卷积网络背景知识"><a href="#卷积网络背景知识" class="headerlink" title="卷积网络背景知识"></a>卷积网络背景知识</h2><p><center><img src="http://od2s78ez3.bkt.clouddn.com/18-8-18/97652655.jpg" alt=""></center><br><div align="center">卷积网络示意图</div><br>典型的卷积网络通过一系列的级联层将输入的图像转换成1000维向量，完成图像的分类识别任务。构建一个卷积网络结构主要包括：卷积层、池化层、全连接层。网络的参数一般用高斯分布初始化，参数的优化通过梯度下降方法。</p></li></ul><p>（注：验证集的作用只是用来验证训练过程是否有效，并不会再次更改网络训练后的权重）</p><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><h3 id="卷积网络可视化"><a href="#卷积网络可视化" class="headerlink" title="卷积网络可视化"></a>卷积网络可视化</h3><p>关于CNNs可视化的相关工作，作者将其分为两个类别，以特征为主（feature-oriented）、以演变为主（evolution-oriented）。</p><ul><li>以特征为主的相关工作</li></ul><p>heatmap的形式使得我们可以明确输入图像的哪些部分对分类结果贡献最大，合成图像方法可以使得我们明确针对特定图像的特征与相关神经元之间的关系，寻找使得特定神经元激活值最大的图像块方法使得我们能够明确特定神经元学习到的特定特征。虽然以上工作能够探索特定网络是怎么工作，但是并不能以一个全局的角度去全面审视整个网络的训练情况。</p><ul><li>网络的训练过程与演变</li></ul><p>典型的方法就是挑选出训练过程中几个几个快照片段（snapshots）。这些方法的局限在于选择合适的训练片段进行比较与观察，而对于分析参数的变化过程，一般的都是利用降维方式映射到二维平面进行展现，但是，目前的这些方法受限于可扩展性，即不能拓展到有效分析大型数据集比如：Imagnet；</p><h3 id="时间序列可视化"><a href="#时间序列可视化" class="headerlink" title="时间序列可视化"></a>时间序列可视化</h3><p>（注：时间序列可视化是第一次接触）<br>两种经典的时间序列可视化方法：</p><ul><li><p>一个图空间放置多个线图（place multiple charts in the same graph space to produce overlapping curves）</p></li><li><p>small multiples方法</p></li></ul><h2 id="需求分析"><a href="#需求分析" class="headerlink" title="需求分析"></a>需求分析</h2><p>基于卷积网络训练过程中普遍存在的问题与现象，与3位专家周期探讨与商量，最后该系统确定了6种亟需解决的需求。</p><ul><li><p>普遍存在问题与现象</p><p>超参数（学习率、批次大小、层的数量、每层滤波器数量、权重衰减、动量等）</p><p>训练过程中监测变化情况：loss函数值、训练/验证集错误率、<strong>权重更新速率</strong>、每一层权重、梯度、激活值分布情况。</p><p>经验告诉我们：随着迭代次数的增加loss值与错误率应该减少，这两者值同时增加或者<br>训练集上面的loss值violent fluctuation则暗示了可能存在问题，验证集与训练集错误率明显存在巨大反差，表明可能出现过拟合现象，同时表明网络模型学习能力有限。权重更新速率期望在10-3附近，权重初始化分布应该遵循高斯分布，训练过程中如果出现梯度爆炸或者消失现象都将是一种不好的信号。激活值的分布随着训练过程高层相对于低层越来越稀疏。</p></li></ul><p>这些经验虽然能够从一定程度去指导网络的调试与优化，但是并不能有针对性解决具体训练过程中出现的特定问题，比如训练过程中模型对于不同类别的性能表现情况等</p><ul><li><p>6种需求</p><ul><li><p>R1 神经元权重信息的多面含义</p></li><li><p>R2 不同层的比较</p></li><li><p>R3 跟踪验证集的分类结果</p></li><li><p>R4 检测出重要的迭代阶段</p></li><li><p>R5 不同验证集类别的表现情况</p></li><li><p>R6  相关性探讨</p><h2 id="系统概览"><a href="#系统概览" class="headerlink" title="系统概览"></a>系统概览</h2><p><img src="http://od2s78ez3.bkt.clouddn.com/18-8-18/84688822.jpg" alt=""><br><div align="center">系统概览图</div><br>DeepTracker是一个基于MEAN.js全栈框架的web应用。后端作者对训练日志进行了层级化组织，制定了特定应用的索引机制进行数据的高效存储便于实时在线分析。对于前端可视化界面，建立了三种协同视图分析，即 Validation view、layer view、correlation view。</p><h2 id="数据收集与分析"><a href="#数据收集与分析" class="headerlink" title="数据收集与分析"></a>数据收集与分析</h2><p>此系统最原始的动机是监控“工业级别”的卷积网络训练过程，所以作者采用的卷积模型是ResNet，使用的数据集ImageNet数据集。面对如此大的一个模型大约需要120个epoch才能达到收敛（batch size=128；iterations=120万次；information volumn = several petabytes；times = 4weeks），作者最终通过与专家商量讨论将信息量控制在ITB之内（采取的措施是：每个epoch内只提取7次迭代变化情况）。</p></li></ul></li></ul><p>作者只记录了两种信息类型：卷积层/全连接层神经元的梯度与权重、图像分类结果信息。</p><p>BatchNormalization层的参数信息作者没有去关注，并且也没有关注每一层的激活值形况，并不是说明这两者信息对于网络训练过程不重要，只是不再本文献的讨论范围之内。</p><p>虽然基于前面的考虑，对数据规模进行了缩小，但是仍然存在数据量大的问题，作者应对的措施是建立数据索引机制。包括：</p><ul><li><p>层统计索引：检索每一次迭代任意一层的统计值（均值、方差）</p></li><li><p>层滤波器索引：列出每一次迭代任意一层的滤波器信息</p></li><li><p>迭代滤波索引：搜索任意一次迭代过程中所有层中改变最大的滤波器</p></li><li><p>类别统计索引：抽取所有迭代过程中每一类别的信息</p></li><li><p>类别图像索引：特定类别的特定图像数据信息</p></li></ul><h2 id="可视化分析"><a href="#可视化分析" class="headerlink" title="可视化分析"></a>可视化分析</h2><h3 id="1-Validation-View"><a href="#1-Validation-View" class="headerlink" title="1. Validation View"></a>1. Validation View</h3><h4 id="R3需求"><a href="#R3需求" class="headerlink" title="R3需求"></a>R3需求</h4><p>再进行可视化的设计的时候，需要说明一个很重要的概念就是：可视化代表的含义即可视编码（Visual Encoding）<br>为了解决需求R3,所有类别图像随着网络迭代的变化总情况。作者设计了如下的可视界面<br><img src="http://od2s78ez3.bkt.clouddn.com/18-8-18/81280187.jpg" alt=""></p><p><div align="center">分类不同类别图像的难易程度</div><br>横轴x: 表示网络训练过程的整个迭代次数，并且用红-&gt;绿的颜色渐变表示错误率情况</p><p>纵轴y： 依据不同类别图像的分类趋势变化情况，利用k-mean聚类算法，将1000种类其分为4个簇，每个簇下方的每次迭代错误率是所有簇类的平均值，采用heatmap这种可视形式展现（一种有效的时间序列可视化方法）</p><h4 id="R5需求"><a href="#R5需求" class="headerlink" title="R5需求"></a>R5需求</h4><p>针对具体某一簇的所有类别图像的分类情况。</p><h4 id="R4异常迭代的发现"><a href="#R4异常迭代的发现" class="headerlink" title="R4异常迭代的发现"></a>R4异常迭代的发现</h4><p>作者提出了<strong>一种检测异常迭代的算法</strong>，并且用倒三角与正三角的可视编码形式表示异常与正常情况。</p><h5 id="异常检测"><a href="#异常检测" class="headerlink" title="异常检测"></a>异常检测</h5><p>此异常算法提出的动机来源于：对于一个动态的分类过程，正常的模式应该是假如把某张图像分类正确之后，应该再不会将此分错，而异常的情况在于某些图片分类情况在分类的过程中一直不确定。</p><h3 id="2-Layer-View"><a href="#2-Layer-View" class="headerlink" title="2. Layer View"></a>2. Layer View</h3><h4 id="R1需求、R2需求"><a href="#R1需求、R2需求" class="headerlink" title="R1需求、R2需求"></a>R1需求、R2需求</h4><p>对于这部分的任务设计，作者将网络的结构与层内的统计信息等放置在一起，主要表现的是神经元权重的统计信息。</p><h3 id="3-Correlation-View"><a href="#3-Correlation-View" class="headerlink" title="3. Correlation View"></a>3. Correlation View</h3><p>这一部分主要考虑滤波器与图像之间的模式，对于某些类别的图像可能检测出数个异常迭代情况，针对每个异常迭代伴随着异常滤波器。那么是否在这多个异常迭代中有着相同的滤波器?</p><h4 id="R6需求"><a href="#R6需求" class="headerlink" title="R6需求"></a>R6需求</h4><p>进一步探究网络的参数是如何影响分类结果。<br><img src="http://od2s78ez3.bkt.clouddn.com/18-8-19/49672354.jpg" alt=""></p><p><div align="center">异常层与异常类别之间的异常滤波器个数展示情况</div><br>如上图所示：纵轴表示在训练过程中出现异常迭代的所有异常层个数，横轴表示所有的异常类别，不同方块颜色深度表示异常滤波器的个数。显然这只是一个抽象的表示。</p><h3 id="4-Cube-Visualization"><a href="#4-Cube-Visualization" class="headerlink" title="4. Cube Visualization"></a>4. Cube Visualization</h3><p>主视角：层级别的角度展示；俯视角：验证数据集角度展示；右视角：相关性分析角度展示</p><p>##后记<br>本文章只是对“DeepTracker:Visualizing the Training Process of Convolutional Neural Networks”文献的简单转述与理解，后续将继续对此文献进行深入挖掘与分析。</p><p>Reference：</p><ol><li><a href="http://www.cse.ust.hk/~dliuae/" target="_blank" rel="noopener">原文作者：刘冬煜</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
      <category term="文献解读" scheme="http://yoursite.com/categories/%E6%96%87%E7%8C%AE%E8%A7%A3%E8%AF%BB/"/>
    
    
      <category term="可解释性AI" scheme="http://yoursite.com/tags/%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7AI/"/>
    
  </entry>
  
  <entry>
    <title>浅谈AdminDemo项目遇到的Debug</title>
    <link href="http://yoursite.com/2018/08/16/%E6%B5%85%E8%B0%88AdminDemo%E9%A1%B9%E7%9B%AE%E9%81%87%E5%88%B0%E7%9A%84Debug/"/>
    <id>http://yoursite.com/2018/08/16/浅谈AdminDemo项目遇到的Debug/</id>
    <published>2018-08-16T12:25:32.000Z</published>
    <updated>2018-08-17T06:01:13.317Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>作为一个工程师，或者一个程序员，可能最讨厌遇到的问题就是：程序出现bug，而作为一个全栈工程师，尤其是前端界面一直不能出现自己想要的效果，此时更是产生焦虑，深深地怀疑“人生”，最糟糕的情况可能会将工程搁置一旁，挫败感增强。</p><p>自己7月末参加了北京大学的可视化暑期学校，在8月上旬借着高温假的时间自己学习了<a href="https://www.zaih.com/mentor/84779110/" target="_blank" rel="noopener">安晓辉老师</a>的<a href="https://edu.csdn.net/course/play/1425/22488" target="_blank" rel="noopener">全栈开发入门课程</a>(MEAN.js)，是一门手把手教学怎么完成一个全后端的工程项目线上课程。每一节过后都有一个实践的实例，最后集成一个大的工程项目:WEB管控系统，此项目开源代码托管于安晓辉老师的github的<a href="https://github.com/foruok/XAdmin" target="_blank" rel="noopener">xdmin</a>项目中。<br><a id="more"></a><br>自己在课程项目当中有两节课就陷入了调试debug过程当中，在实现AdminDemo的作业当中，代码也是跟着视频进行全程敲写的，但是最终运行的前端界面一直出现不了自己的想要结果，查找原因，最终调试查找，记录如下：</p><p>（注：以下两处都是自己在解决bug之后，重现bug，并以记录）</p><h2 id="Debug1：左侧条目一直不能成功显示"><a href="#Debug1：左侧条目一直不能成功显示" class="headerlink" title="Debug1：左侧条目一直不能成功显示"></a>Debug1：左侧条目一直不能成功显示</h2><p><img src="http://od2s78ez3.bkt.clouddn.com/18-8-16/10591553.jpg" alt=""></p><p><div align="center">BUG显示界面</div><br>出现上述情况原因如下：<br><img src="http://od2s78ez3.bkt.clouddn.com/18-8-16/56627235.jpg" alt=""></p><p><div align="center">code工程界面</div><br>代码：</p><pre><code>$scope.menus =[];</code></pre><p>语句的分号使用了<strong>中文分号“；”</strong>，使得前端出现莫名其妙的问题，这个问题之前在使用百度开源的Echarts前端可视化工具也曾遇到过，如此一个小的中英文问题使得前端效果一直出现不了。最终显示界面如下所示：<br><img src="http://od2s78ez3.bkt.clouddn.com/18-8-16/3148652.jpg" alt=""></p><p><div align="center">正常前端界面</div></p><h2 id="Debug2：左侧条目“手风琴折叠样式”不能成功显示"><a href="#Debug2：左侧条目“手风琴折叠样式”不能成功显示" class="headerlink" title="Debug2：左侧条目“手风琴折叠样式”不能成功显示"></a>Debug2：左侧条目“手风琴折叠样式”不能成功显示</h2><p>当使用手风琴折叠样式的前端效果，课程提到需要引入以下几个新的js类库：</p><ul><li><p>angular-1.4.3.min.js</p></li><li><p>angular-animate.js</p></li><li><p>ui-bootstrap-tpls-0.13.3.min.js</p></li></ul><p>正如作者提到下载UI组件的“ui-bootstrap-tpls-0.13.3.min.js”库使用github项目中的gh-pages分支<sup>[1]</sup><br><img src="http://od2s78ez3.bkt.clouddn.com/18-8-16/22700203.jpg" alt=""><sup>[2]</sup></p><p>运行之后前端显示如下：<br><img src="http://od2s78ez3.bkt.clouddn.com/18-8-16/71961703.jpg" alt=""></p><p><div align="center">BUG显示界面</div><br>debug信息如下：<br><img src="http://od2s78ez3.bkt.clouddn.com/18-8-16/24318593.jpg" alt=""></p><p><div align="center">终端运行信息与浏览器调试信息</div><br>运行浏览器调试信息第一行显示如下：<br><img src="http://od2s78ez3.bkt.clouddn.com/18-8-16/68736671.jpg" alt=""></p><p><div align="center">accordion-group.html信息</div><br>由于自己第一次接触ui-bootstrap组件的类库，当时并不清楚出现这个bug的原因所在，然后google此错误，大部分说在自己创建的js中没有使用’ui.bootstrap’,但是自己确实是按照视频当中的语句：</p><pre><code>angular.module(x-admin,[&apos;ui.bootstrap&apos;,&apos;ngAnimate&apos;])</code></pre><p>最后自己的解决方案是将<a href="https://github.com/foruok/XAdmin" target="_blank" rel="noopener">xdmin</a>项目的相关js库替换自己的js库文件，然后再次运行程序，前端界面图下：<br><img src="http://od2s78ez3.bkt.clouddn.com/18-8-16/25667278.jpg" alt=""></p><p><div align="center">正常显示“手风琴折叠样式”界面</div><br>当看到前端界面能够如愿以偿的展示自己的想要结果，顿时感觉生活还是那么得美好！最终找到出现bug原因在于自己下载使用的“ui-bootstrap-tpls-0.13.3.min.js”库有问题：<br><img src="http://od2s78ez3.bkt.clouddn.com/18-8-16/24082741.jpg" alt=""><br>正常的“ui-bootstrap-tpls-0.13.3.min.js”库如下所示：<br><img src="http://od2s78ez3.bkt.clouddn.com/18-8-16/54100499.jpg" alt=""><br>对比可以发现确实是少了一些代码片段，导致前端报错：<strong>Failed to load resource</strong>.当自己再次查找下载源得时候发现原始类库是有相应得代码片段，也不知道自己为什么下载的ui-bootstrap-tpls-0.13.3.min.js竟然少<strong>手风琴折叠样式</strong>代码。</p><h2 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h2><p>作为全栈的一个初学者，上述两个bug错误可能是每一个入门者难免遇到的问题，遂记录下来，若能够对后入坑的同学起到帮助作用，甚是欣慰。深刻体会到前端界面不能如愿以偿的显示自己想要的结果，有可能就是自己的一个很小的错误，然后花费大量的时间去找此问题，过程是多么地令人揪心与烦躁。不妨在遇到bug问题，暂时解决不了可以先放放，比如去跑个步，回来再理清思路，可能就解决了。</p><p>Reference：</p><ol><li><a href="https://www.cnblogs.com/MuYunyun/p/6082359.html" target="_blank" rel="noopener">github的gh-pages分支展示自己的项目</a></li><li><a href="https://blog.csdn.net/foruok/article/details/48173377" target="_blank" rel="noopener">Node.js开发入门-引入UIbootstrap</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作为一个工程师，或者一个程序员，可能最讨厌遇到的问题就是：程序出现bug，而作为一个全栈工程师，尤其是前端界面一直不能出现自己想要的效果，此时更是产生焦虑，深深地怀疑“人生”，最糟糕的情况可能会将工程搁置一旁，挫败感增强。&lt;/p&gt;
&lt;p&gt;自己7月末参加了北京大学的可视化暑期学校，在8月上旬借着高温假的时间自己学习了&lt;a href=&quot;https://www.zaih.com/mentor/84779110/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;安晓辉老师&lt;/a&gt;的&lt;a href=&quot;https://edu.csdn.net/course/play/1425/22488&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;全栈开发入门课程&lt;/a&gt;(MEAN.js)，是一门手把手教学怎么完成一个全后端的工程项目线上课程。每一节过后都有一个实践的实例，最后集成一个大的工程项目:WEB管控系统，此项目开源代码托管于安晓辉老师的github的&lt;a href=&quot;https://github.com/foruok/XAdmin&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;xdmin&lt;/a&gt;项目中。&lt;br&gt;
    
    </summary>
    
      <category term="可视化" scheme="http://yoursite.com/categories/%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
    
      <category term="全栈" scheme="http://yoursite.com/tags/%E5%85%A8%E6%A0%88/"/>
    
  </entry>
  
  <entry>
    <title>CSAPP课程（Computer Systems A Programmer&#39;s Perspective)</title>
    <link href="http://yoursite.com/2018/08/09/CSAPP%E8%AF%BE%E7%A8%8B/"/>
    <id>http://yoursite.com/2018/08/09/CSAPP课程/</id>
    <published>2018-08-09T12:19:46.943Z</published>
    <updated>2018-08-09T12:19:46.953Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
      <category term="Project" scheme="http://yoursite.com/categories/Project/"/>
    
    
      <category term="Computer Systems" scheme="http://yoursite.com/tags/Computer-Systems/"/>
    
  </entry>
  
  <entry>
    <title>机器学习的可解译性</title>
    <link href="http://yoursite.com/2018/07/31/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7/"/>
    <id>http://yoursite.com/2018/07/31/机器学习的可解释性/</id>
    <published>2018-07-31T09:09:12.022Z</published>
    <updated>2017-08-31T01:30:38.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><img src="/2018/07/31/机器学习的可解释性/timg.jpg" title="Machine Learning"><blockquote><p>Interpretation is the process of giving<br>explanations<br>解译性指的是给出可解释的过程</p></blockquote><p>即三个问题</p><ol><li>为什么要给出解译性？</li><li>通过什么途径给出可解释性呢？</li><li>我们又如何去评价这是一种好的可解释方法？</li></ol><h5 id="可解释模型的分类（既不是互相排斥-mutually-exclusive-的分类，也不是文献的罗列-exhaustive-list-）"><a href="#可解释模型的分类（既不是互相排斥-mutually-exclusive-的分类，也不是文献的罗列-exhaustive-list-）" class="headerlink" title="可解释模型的分类（既不是互相排斥(mutually exclusive)的分类，也不是文献的罗列(exhaustive list)）"></a>可解释模型的分类（既不是互相排斥(mutually exclusive)的分类，也不是文献的罗列(exhaustive list)）</h5><p>方法：</p><ol><li><p>搭建模型之前</p><ul><li>数据探索的可视（KMeans KNN 等聚类算法）</li></ul></li><li><p>搭建模型过程当中</p><ul><li>基于规则/基于属性(特征)</li><li>基于实例</li><li>从稀疏角度</li><li>从单调角度（借助制定区间形式定义函数变化的趋势（user specified lattice))</li></ul></li><li><p>给定模型之后(比如深度学习)</p><ul><li>敏感性分析，基于梯度的方法（sensitivity analysis）此方法来自金融风险（自变量与因变量关系）领域</li><li>模型的替换</li><li>隐层、中间层的观察</li></ul><p>评价<br>从不同角度去评价！可参考原文</p><p>Reference：</p><ol><li><a href="https://link.zhihu.com/?target=http://people.csail.mit.edu/beenkim/icml_tutorial.html" target="_blank" rel="noopener">Tutorial on Interpretable Machine Learning by Been Kim</a></li><li><a href="https://www.zhihu.com/question/60374968" target="_blank" rel="noopener">ICML 2017上哪些论文值得关注</a></li></ol></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>解析word文档表格</title>
    <link href="http://yoursite.com/2018/07/31/%E8%A7%A3%E6%9E%90word%E6%96%87%E6%A1%A3/"/>
    <id>http://yoursite.com/2018/07/31/解析word文档/</id>
    <published>2018-07-31T09:09:12.011Z</published>
    <updated>2018-05-14T15:15:10.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><pre><code># -*- coding: utf-8 -*-# from __future__ import print_functionimport sysreload(sys)sys.setdefaultencoding(&apos;utf-8&apos;)import win32comfrom win32com.client import Dispatch, constantsw = win32com.client.Dispatch(&apos;Word.Application&apos;)w.Visible = 1w.Documents.Open( FileName = &apos;D:\muzi\\37.docx&apos;)  #路径不能存在中文doc = w.ActiveDocumentcount = doc.Tables.Count  #统计整个word文档有多少个表a = doc.Tables(1).rows.Count # 返回表格的行数b = doc.Tables(1).columns.Count # 返回表格的列数import collectionsContent = collections.OrderedDict()for num in range (count):    for i in range (doc.Tables(num+1).rows.Count):        key = re.sub(r&apos;\r\x07&apos;,&apos;&apos;,doc.Tables(num+1).Cell(Row= i+1,Column = 1).Range.Text) //正则化去除特殊符号        value = re.sub(r&apos;\r\07&apos;,&apos;&apos;,doc.Tables(num+1).Cell(Row=i+1,Column = 2).Range.Text)         Content[key] = value    filename = &apos;D:\muzi\m&apos;      import json    with open(filename+&apos;.json&apos;,&apos;a&apos;) as outfile:    //打开已存在的文件        json.dump(Content,outfile,ensure_ascii = False) //字典内容写入json文件中        outfile.write(&apos;\n&apos;)</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>opencv离线安装问题</title>
    <link href="http://yoursite.com/2018/07/31/opencv%E5%AE%89%E8%A3%85%E9%97%AE%E9%A2%98/"/>
    <id>http://yoursite.com/2018/07/31/opencv安装问题/</id>
    <published>2018-07-31T09:09:11.991Z</published>
    <updated>2018-04-24T04:11:06.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="step-0"><a href="#step-0" class="headerlink" title="step 0"></a>step 0</h3><p>下载源文件 opencv-2.4.11.zip<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ unzip opencv-2.4.11.zip</span><br></pre></td></tr></table></figure></p><p>在解压路径下面进行编译<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cmake CMakeLists.txt -DWITH_CUDA=ON -DCUDA_ARCH_BIN=<span class="string">"3.2"</span> -DCUDA_ARCH_PTX=<span class="string">""</span> -DBUILD_TESTS=OFF -DBUILD_PERF_TESTS=OFF -DBUILD_NEW_PYTHON_SUPPORT=ON</span><br><span class="line"></span><br><span class="line">make -j4</span><br></pre></td></tr></table></figure></p><a id="more"></a><p>报错0：<br><img src="/2018/07/31/opencv安装问题/161227210742477.png"><br>opencv与cuda8.0不兼容导致，～/opencv/modules/cudalegacy/src/graphcuts.cpp文件内容：<br><img src="/2018/07/31/opencv安装问题/161227210742478.png"><br>OK！重新编译</p><h3 id="step-1"><a href="#step-1" class="headerlink" title="step 1"></a>step 1</h3><p>安装源文件<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ make install</span><br></pre></td></tr></table></figure></p><p>ok 安装成功！<br>opencv默认安装位置为“/usr/local”下lib、bin、include等目录</p><h3 id="step-2"><a href="#step-2" class="headerlink" title="step 2"></a>step 2</h3><p>使用 python import cv2<br>错误1：<br>没有此模块，需要将opencv安装的源文件下面编译生成的共享文件库cv2.so拷贝至使用的python 下面。<br>本人使用的python环境是 /home/muzi-18/anaconda2/lib/python2.7/site-packages<br>错误 2：<br>libstdc++so.6 version ‘GLIBCXX_3.4.21’not found<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ cp /usr/lib/x86_64-linux-gnu/libstdc++.so.6  /home/muzi-18/anaconda2/lib/libstdc++.so.6</span><br></pre></td></tr></table></figure></p><p>ok!<br>python<br> import cv2<br>测试成功！</p><p>Reference：<a href="http://www.linuxidc.com/Linux/2016-12/138870.htm" target="_blank" rel="noopener">http://www.linuxidc.com/Linux/2016-12/138870.htm</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;step-0&quot;&gt;&lt;a href=&quot;#step-0&quot; class=&quot;headerlink&quot; title=&quot;step 0&quot;&gt;&lt;/a&gt;step 0&lt;/h3&gt;&lt;p&gt;下载源文件 opencv-2.4.11.zip&lt;br&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ unzip opencv-2.4.11.zip&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;在解压路径下面进行编译&lt;br&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;cmake CMakeLists.txt -DWITH_CUDA=ON -DCUDA_ARCH_BIN=&lt;span class=&quot;string&quot;&gt;&quot;3.2&quot;&lt;/span&gt; -DCUDA_ARCH_PTX=&lt;span class=&quot;string&quot;&gt;&quot;&quot;&lt;/span&gt; -DBUILD_TESTS=OFF -DBUILD_PERF_TESTS=OFF -DBUILD_NEW_PYTHON_SUPPORT=ON&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;make -j4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Software Install" scheme="http://yoursite.com/categories/Software-Install/"/>
    
    
      <category term="Linux" scheme="http://yoursite.com/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>写给自己的一封信</title>
    <link href="http://yoursite.com/2018/07/31/%E5%86%99%E7%BB%99%E8%87%AA%E5%B7%B1%E7%9A%84%E4%B8%80%E5%B0%81%E4%BF%A1/"/>
    <id>http://yoursite.com/2018/07/31/写给自己的一封信/</id>
    <published>2018-07-31T09:09:11.981Z</published>
    <updated>2018-08-09T13:12:23.511Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote><p>十年饮冰, 难凉热血, 梦还在,黄苍终会眷顾.         –梁启超</p></blockquote><p>北京的周五依旧是行人匆匆,夹杂着兮兮小雨,更是显得匆忙与急促.打着雨伞赶着去实验室,继续完成自己调不完的bug,写不完的程序.真是应了实验室同学的调侃”沐风栉雨搞科研”.<br>惯例周五的实验室必定是空空荡荡,无意在微信朋友圈看到好友江转发了一篇关于NBA比赛报道的文章,若不是他配有文字描述”如果足够努力,最差的结果不就是大器晚成”, 还以为又是司空见惯的NBA球场上那些”飞禽猛兽”不可思议的表现.充满着疑问点开了由”虎扑体育”公众号推送的题为”你生涯最高光的时刻是总决赛MVP吗?而我,就是现在了”.看完文章之后,已然哭成一个傻逼样,心情久久不能平复.<br><a id="more"></a><br>文章讲述了一个主人公名叫”安德烈-英格拉姆”在发展联盟打了整整10年球,度过了长达10年之久的每年领着卑微的2.6W的年薪,同时兼职当家教补贴两个女儿的生活费,终于在自己32岁时迎来了NBA生涯首秀.他在赛后回忆说<strong>“这里的球场看起来更加明亮一些,太出乎意料了,我就是感觉到了一股电流流遍全身,太美妙了热情的观众,耀眼的灯光,这真的是一辈子一次的景象</strong><br>这不就是电影”当幸福来敲门”的克里斯活生生的例子啊!吓得我虎躯一震,生活中真有为梦想而坚持的人? 赶紧打开百度浏览器搜索了关键词<strong>“安德烈-英格拉姆”</strong>,有一篇知乎名为”如何评价「湖人与后卫安德烈-英格拉姆签约至本赛季结束」?”这个鲜活有生命力的运动员形象慢慢在我的脑海中丰富起来,认真看完了网友的所有评论,”大道至简”网友评论道</p><blockquote><p>我很尊重为了梦想在打拼的人们,看到魔术师和沃顿他们轻松的笑说”祝贺”,他激动的只会说”感谢”,我会想到千千万万个他们,像他一样,每天默默努力,终于有朝一日成功的那种激动带点傻呵呵的笑和不知所措,真心尊敬与祝福你,祝贺你的梦想实现,就像你的T恤上的一样”always in pursuit”! ,当你认真踏上球场的那一刻,请相信,全世界都在为你骄傲,加油!<br><img src="http://od2s78ez3.bkt.clouddn.com/18-4-13/81508159.jpg" alt=""></p></blockquote><p>正如毛姆小说”月亮与六便士”中的查尔斯先生,在追逐梦想的路上,他过得穷困潦倒,并且一再遭受命运的摧残,不同的是查尔斯先生到死都没有亲眼看到自己的一幅画能够价值连城,而安德烈-英格拉姆在自己32岁终于踏入了斯台普斯中心(Staples Center).实现了属于自己追逐了十年的梦想.正如书中所说:</p><blockquote><p>难道做自己最想做的事,生活在让你感到舒服的环境里,让你的内心得到安宁是糟践自己吗?难道成为年入上万英镑的外科医生,娶得如花美眷就算是成功吗?</p></blockquote><p>至于自己为什么会哭的一塌糊涂,或许只有相同经历的人才能深深的感受到安德烈的那种无奈与坚毅,对于自己而言,求学之路仍未结束,应该是到了深水区与艰难区,高中因为自己的后知后觉,虽然无心插柳考上了市重点中学,但依旧没能鲤鱼跃龙门,选择了fu读之路,就靠着”黑曼巴”精神顺利步入大学之门,进入大学的第一天目标就非常明确,一定要借着”西北农林科技大学”985这个平台跳出去,完成自己”梦想”,正如我所愿,4年的奋斗换来”中科院电子所”的直博入学书.<br>本科的专业是EE,如今要转向CS,其中的自卑与不知所措,让我一下子失去了方向,没有了曾经的优秀感,还好自己已经行动起来,凭借着自学能力与领悟能力,全方位的去弥补自己知识的不足与能力的欠缺,一切都还在路上,正如 吴军老师在”浪潮之巅”所讲:处在这个”Deep Learning”浪潮之中,要有所作为,不要再次成为观潮者. 不忘初心,砥砺前行!<br>最后以习总书记2018年在亚洲博鳌经济论坛的警句结束啰啰嗦嗦的文章</p><blockquote><p>积土而为山, 积水而为海,幸福和美好的未来不会自己出现,成功属于勇毅而笃行的人</p></blockquote><p>这不正好印证了我的两所大学的校训了嘛,不说了,赶紧去写程序…<br>注:<br>西北农林科技大学 校训 : 诚朴勇毅<br>中国科学院大学 校训: 博学笃志, 格物明德</p><p>Reference:<br><a href="https://www.washingtonpost.com/news/dc-sports-bog/wp/2018/04/10/after-a-decade-in-the-minors-this-32-year-old-finally-got-called-up-by-the-lakers/?noredirect=on&amp;utm_term=.77ed308caff9" target="_blank" rel="noopener">This gray-haired 32-year-old rookie spent 10 years in the minors. His NBA debut was too good to believe</a></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;十年饮冰, 难凉热血, 梦还在,黄苍终会眷顾.         –梁启超&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;北京的周五依旧是行人匆匆,夹杂着兮兮小雨,更是显得匆忙与急促.打着雨伞赶着去实验室,继续完成自己调不完的bug,写不完的程序.真是应了实验室同学的调侃”沐风栉雨搞科研”.&lt;br&gt;惯例周五的实验室必定是空空荡荡,无意在微信朋友圈看到好友江转发了一篇关于NBA比赛报道的文章,若不是他配有文字描述”如果足够努力,最差的结果不就是大器晚成”, 还以为又是司空见惯的NBA球场上那些”飞禽猛兽”不可思议的表现.充满着疑问点开了由”虎扑体育”公众号推送的题为”你生涯最高光的时刻是总决赛MVP吗?而我,就是现在了”.看完文章之后,已然哭成一个傻逼样,心情久久不能平复.&lt;br&gt;
    
    </summary>
    
      <category term="人生" scheme="http://yoursite.com/categories/%E4%BA%BA%E7%94%9F/"/>
    
    
      <category term="思考" scheme="http://yoursite.com/tags/%E6%80%9D%E8%80%83/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://yoursite.com/2018/07/31/Pytorch%E6%95%99%E7%A8%8B(%E4%B8%80)/"/>
    <id>http://yoursite.com/2018/07/31/Pytorch教程(一)/</id>
    <published>2018-07-31T09:09:11.975Z</published>
    <updated>2017-10-18T21:01:52.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><h2 id="PYTORCH-系列教程（一）"><a href="#PYTORCH-系列教程（一）" class="headerlink" title="PYTORCH 系列教程（一）"></a>PYTORCH 系列教程（一）</h2><p>Author：muzi</p><p><title><center>201_torch_numpy</center></title><br><img src="http://od2s78ez3.bkt.clouddn.com/17-10-19/33138087.jpg" alt=""><br><img src="http://od2s78ez3.bkt.clouddn.com/17-10-19/36291573.jpg" alt=""></p><ol><li><p>将numpy数组转换为张量，或者张量转换为数组<br>(convert numpy to tensor or vise versa 反之亦然)</p><p> code<br> import torch<br> import numpy as np<br> np_data = np.arange(6).reshape((2, 3))<br> print np_data<br> torch_data = torch.from_numpy(np_data)<br> print torch_data<br> tensor2array = torch_data.numpy()<br> print tensor2array<br>结果：</p><p> [[0 1 2]<br>  [3 4 5]]</p><p>  0  1  2<br>  3  4  5<br> [torch.LongTensor of size 2x3]</p><p> [[0 1 2]<br>  [3 4 5]]</p></li><li><p>元素取绝对值</p><p> code<br> data = [-1, -2, 1, 2]<br> print data<br> tensor = torch.FloatTensor(data)<br> print tensor<br> print np.abs(data)<br> print torch.abs(tensor)<br>结果：</p><p> [-1, -2, 1, 2]</p><p> -1<br> -2<br>  1<br>  2<br> [torch.FloatTensor of size 4]</p><p> [1 2 1 2]</p><p>  1<br>  2<br>  1<br>  2<br> [torch.FloatTensor of size 4]</p></li><li><p>元素取sin值</p><p> code<br> print np.sin(data)<br> print torch.sin(tensor)<br>结果：</p><p> [-0.84147098 -0.90929743  0.84147098  0.90929743]</p><p> -0.8415<br> -0.9093<br>  0.8415<br>  0.9093<br> [torch.FloatTensor of size 4]</p></li><li><p>张量元素取sigmoid值</p><p> code<br> print tensor.sigmoid()<br>结果</p><p>  0.2689<br>  0.1192<br>  0.7311<br>  0.8808<br> [torch.FloatTensor of size 4]</p></li><li><p>张量元素取指数值</p><p> code<br> print tensor.exp()<br>结果：</p><p> 0.3679<br> 0.1353<br> 2.7183<br> 7.3891<br>  [torch.FloatTensor of size 4]</p></li><li><p>元素取平均值</p><p> code<br> print np.mean(data)<br> print torch.mean(tensor)<br>结果：</p><p> 0.0<br> 0.0<br>7.列表，张量的矩阵运算</p><p> data = [[1,2], [3,4]]<br> print data<br> print type(data)<br> tensor = torch.FloatTensor(data)<br> print np.matmul(data,data)<br> print type(np.matmul(data,data))<br> print torch.mm(tensor,tensor)<br>结果：</p><p> [[1, 2], [3, 4]]<br> <type 'list'=""></type></p><h1 id="数据类型是list，可以通过matmul计算list的矩阵运算。"><a href="#数据类型是list，可以通过matmul计算list的矩阵运算。" class="headerlink" title="数据类型是list，可以通过matmul计算list的矩阵运算。"></a>数据类型是list，可以通过matmul计算list的矩阵运算。</h1><p> [[ 7 10]<br>  [15 22]]<br> <type 'numpy.ndarray'=""><br>   7  10<br>  15  22<br> [torch.FloatTensor of size 2x2]</type></p></li><li><p>数组的矩阵乘法操作；张量的內积运算</p><p> code<br> data = np.array(data)<br> print data<br> print type(data) #数据类型是数组，可以通过dot操作计算矩阵运算<br> print data.dot(data)<br> tensor = torch.FloatTensor(data)<br> print tensor<br> #print tensor.dot(tensor) #不正确的操作，因为维度不匹配<br> data1 = [1,2,3,4]<br> print type(data1)<br> print type(data1)<br> tensor = torch.FloatTensor(data1)<br> print tensor<br> print tensor.dot(tensor) #正确的操作 进行內积运算<br>结果：</p><p> [[1 2]<br>  [3 4]]<br> <type 'numpy.ndarray'=""><br> [[ 7 10]<br>  [15 22]]</type></p><p>  1  2<br>  3  4<br> [torch.FloatTensor of size 2x2]</p><p> [1, 2, 3, 4]</p> <type 'list'=""><p>  1<br>  2<br>  3<br>  4<br> [torch.FloatTensor of size 4]</p><p> 30.0</p></type></li><li><p>张量对应元素相乘</p><p> data =[[1,2],[3,4]]<br> print data<br> print type(data)<br> data = np.array(data)<br> print data        # 数据类型：列表与数组区别<br> print type(data)<br> tensor = torch.FloatTensor(data)<br> print tensor.mm(tensor)<br> print tensor*tensor          # 张量元素的对应乘积<br> data1 = np.arange(1,5)<br> data2 = np.array([1,2,3,4])  # 两种方式 产生(n,)的数组<br> print data1<br> print data2<br> print type(data1)<br> print data1.shape<br> tensor_1D = torch.FloatTensor(data1)<br> tensor_1D_ = torch.from_numpy(data2)   #两种方式 将（n,）的数组转换为 1-D张量<br> print tensor_1D<br> print tensor_1D.dot(tensor_1D)         # 1-D张量类型数据的內积：对应元素相乘求和<br>结果：</p><p> [[1, 2], [3, 4]]<br> <type 'list'=""><br> [[1 2]<br>  [3 4]]<br> <type 'numpy.ndarray'="">    #列表与数组的区别</type></type></p><p>   7  10<br>  15  22<br> [torch.FloatTensor of size 2x2]   #张量矩阵乘法</p></li></ol><pre><code>  1   4  9  16[torch.FloatTensor of size 2x2]  #张量对应元素相乘[1 2 3 4][1 2 3 4]&lt;type &apos;numpy.ndarray&apos;&gt;(4,) 1 2 3 4[torch.FloatTensor of size 4]&lt;class &apos;torch.FloatTensor&apos;&gt;30.0</code></pre><p>Reference :<br><a href="https://github.com/MorvanZhou/PyTorch-Tutorial" target="_blank" rel="noopener">莫烦：pytorch教程</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://yoursite.com/2018/07/31/hello-world/"/>
    <id>http://yoursite.com/2018/07/31/hello-world/</id>
    <published>2018-07-31T08:25:38.829Z</published>
    <updated>2018-07-31T08:25:38.829Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Generative Adversarial Networks</title>
    <link href="http://yoursite.com/2016/11/27/2016-11-27-gans/"/>
    <id>http://yoursite.com/2016/11/27/2016-11-27-gans/</id>
    <published>2016-11-27T02:00:00.000Z</published>
    <updated>2018-04-24T18:09:04.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="http://7xritj.com1.z0.glb.clouddn.com/public/16-11-27/13513902.jpg" alt=""><br><a id="more"></a></p><p>人工智能目前的核心目标应该是赋予机器自主理解我们所在世界的能力。对于人类来说，我们对这个世界所了解的知识可能很快就会忘记，比如我们所处的三维环境中，物体能够交互，移动，碰撞；什么动物会飞，什么动物吃草等等。这些巨大的并且不断扩大的信息现在是很容易被机器获取的，问题的关键是怎么设计模型和算法让机器更好的去分析和理解这些数据中所蕴含的宝藏。</p><p><code>Generative models</code>(生成模型)现在被认为是能够实现这一目标的最有前景的方法之一。<code>Generative models</code>通过输入一大堆特定领域的数据进行训练（比如图像，句子，声音等）来使得模型能够产生和输入数据相似的输出。这一直觉的背后可以由下面名言阐述。</p><blockquote><p>“What I cannot create, I do not understand.”  —Richard Feynman</p></blockquote><p>生成模型由一个参数数量比训练数据少的多神经网络构成，所以生成模型为了能够产生和训练数据相似的输出就会迫使自己去发现数据中内在的本质内容。训练<code>Generative models</code>的方法有几种，在这里我们主要阐述其中的<code>Adversarial Training</code>（对抗训练）方法。</p><h2 id="Adversarial-Training"><a href="#Adversarial-Training" class="headerlink" title="Adversarial Training"></a>Adversarial Training</h2><p>上文说过Adversarial Training是训练生成模型的一种方法。为了训练生成模型，Adversarial Training提出一种<code>Discriminative Model</code>(判别模型)来和生成模型产生对抗，下面来说说<code>Generative models</code> $G(z)$ 和 <code>Discriminative Model</code> $D(x)$ 是如何相互作用的。</p><ul><li>生成模型的目标是模仿输入训练数据, 通过输入一个随机噪声来产生和训练数据相似的样本；</li><li>判别模型的目标就是判断生成模型产生的样本和真实的输入样本之间的相似性。</li></ul><p>其中生成模型和判别模型合起来的框架被称为<code>GAN</code>网络。通过下图我们来理清判别模型和生成模型之间的输入输出关系：生成模型通过输入随机噪声 $z(z \sim p_z)$ 产生合成样本；而判别模型通过分别输入真实的训练数据和生成模型的训练数据来判断输入的数据是否真实。</p><center><br><img src="https://culurciello.github.io/assets/unsup/gan_simple.svg" alt=""><br></center><p>描述了<code>GAN</code>的网络结构，但它的优化目标是什么？怎么就可以通过训练使得生成模型能够产生和真实数据相似的输出？优化的目标其实很简单，简单来说就是：</p><ul><li>判别模型努力的想把真实的数据预测为<code>1</code>，把生成的数据预测为<code>0</code>；</li><li>而生成模型的奋斗目标则为‘我’要尽力的让判别模型对‘我’生成的数据预测为<code>1</code>，让判别模型分不清‘我’产生的数据和真实数据之间的区别，从而达到‘以假乱真’的效果。</li></ul><p>下面用形式化说明下如果训练GAN网络, 先定义一些参数：</p><table><thead><tr><th style="text-align:center">参数</th><th style="text-align:center">含义</th></tr></thead><tbody><tr><td style="text-align:center">$p_z$</td><td style="text-align:center">输入随机噪声 $z$ 的分布</td></tr><tr><td style="text-align:center">$p_{data}$</td><td style="text-align:center">未知的输入样本的数据分布</td></tr><tr><td style="text-align:center">$p_g$</td><td style="text-align:center">生成模型的输出样本的数据分布，GAN的目标就是要$p_g=p_{data}$</td></tr></tbody></table><p>训练判别模型 $D(x)$ 的目标：</p><ol><li>对每一个输入数据 $x \sim p_{data}$ 要使得 $D(x)$ 最大；</li><li>对每一个输入数据 $x \nsim p_{data}$ 要使得 $D(x)$ 最小。</li></ol><p>训练生成模型 $G(z)$ 的目标是来产生样本来欺骗判别模型 $D$, 因此目标为最大化 $D(G(z))$，也就是把生成模型的输出输入到判别模型，然后要让判别模型预测其为真实数据。同时，最大化 $D(G(z))$ 等同于最小化 $1-D(G(z))$，因为 $D$ 的输出是介于0到1之间的，真实数据努力预测为1，否则为0。</p><p>所以把生成模型和判别模型的训练目标结合起来，就得到了<code>GAN</code>的优化目标：</p><p>$$\min_G \max_D {\mathbb E}<em>{x\sim p</em>{\rm data}} \log D(x)+{\mathbb E}_{z\sim p_z}[\log (1-D(G(z)))] $$</p><p>总结一下上面的内容，GAN启发自博弈论中的二人零和博弈，在二人零和博弈中，两位博弈方的利益之和为零或一个常数，即一方有所得，另一方必有所失。GAN模型中的两位博弈方分别由生成模型和判别模型充当。生成模型G捕捉样本数据的分布，判别模型是一个二分类器，估计一个样本来自于训练数据（而非生成数据）的概率。G和D一般都是非线性映射函数，例如多层感知机、卷积神经网络等。生成模型的输入是一些服从某一简单分布（例如高斯分布）的随机噪声z，输出是与训练图像相同尺寸的生成图像。向判别模型D输入生成样本，对于D来说期望输出低概率（判断为生成样本），对于生成模型G来说要尽量欺骗D，使判别模型输出高概率（误判为真实样本），从而形成竞争与对抗。</p><h2 id="GAN实现"><a href="#GAN实现" class="headerlink" title="GAN实现"></a>GAN实现</h2><p>一个简单的一维数据GAN网络的tensorflow实现:<a href="https://github.com/ericjang/genadv_tutorial" target="_blank" rel="noopener">genadv_tutorial</a><br>其一维训练数据分布如下所示，是一个均值-1， $\sigma =1$ 的正态分布。</p><center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/public/16-11-24/4360345.jpg" alt=""><br></center><p>我们结合代码和上面的理论内容来分析下GAN的具体实现，判别模型的优化目标为最大化下式，其中 $D_1(x)$ 表示判别真实数据, $D_2(G(z))$ 表示对生成的数据进行判别， 其中 $D_1$ 和 $D_2$ 是共享参数的， 也就是说是同一个判别模型。</p><p>$$\log(D_1(x))+\log(1-D_2(G(z)))$$</p><p>对应的python代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">batch=tf.Variable(<span class="number">0</span>)</span><br><span class="line">obj_d=tf.reduce_mean(tf.log(D1)+tf.log(<span class="number">1</span>-D2))</span><br><span class="line">opt_d=tf.train.GradientDescentOptimizer(<span class="number">0.01</span>)</span><br><span class="line">              .minimize(<span class="number">1</span>-obj_d,global_step=batch,var_list=theta_d)</span><br></pre></td></tr></table></figure></p><p>为了优化 $G$, 我们想要最大化 $D_2(x’)$(成功欺骗 $D$ )，因此 $G$ 的优化函数为：</p><p>$$\log(D_2(G(z)))$$</p><p>对应的python代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">batch=tf.Variable(<span class="number">0</span>)</span><br><span class="line">obj_g=tf.reduce_mean(tf.log(D2))</span><br><span class="line">opt_g=tf.train.GradientDescentOptimizer(<span class="number">0.01</span>)</span><br><span class="line">              .minimize(<span class="number">1</span>-obj_g,global_step=batch,var_list=theta_g)</span><br></pre></td></tr></table></figure><p>定义好优化目标后，下面就是训练的主要代码了：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Algorithm 1, GoodFellow et al. 2014</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(TRAIN_ITERS):</span><br><span class="line">    x= np.random.normal(mu,sigma,M) <span class="comment"># sample minibatch from p_data</span></span><br><span class="line">    z= np.random.random(M)  <span class="comment"># sample minibatch from noise prior</span></span><br><span class="line">    sess.run(opt_d, &#123;x_node: x, z_node: z&#125;) <span class="comment"># update discriminator D</span></span><br><span class="line">    z= np.random.random(M) <span class="comment"># sample noise prior</span></span><br><span class="line">    sess.run(opt_g, &#123;z_node: z&#125;) <span class="comment"># update generator G</span></span><br></pre></td></tr></table></figure></p><p>下面是实验的结果，左图是训练之间的数据，可以看到生成数据的分布和训练数据相差甚远；右图是训练后的数据分析，生成数据和训练数据的分布接近了很多，且此时判别模型的输出分布在0.5左右，说明生成模型顺利的欺骗到判别模型。</p><figure><br>    <img src="http://7xritj.com1.z0.glb.clouddn.com/public/16-11-29/64984825.jpg"><br><br></figure><h2 id="DCGAN"><a href="#DCGAN" class="headerlink" title="DCGAN"></a>DCGAN</h2><p>GAN的一个改进模型就是DCGAN。这个网络的生成模型的输入为一个100个符合均匀分布的随机数（通常被称为<code>code</code>），然后产生输出为64x64x3的输出图像(下图中 $G(z)$ ), 当<code>code</code>逐渐递增时，生成模型输出的图像也逐渐变化。下图中的生产模型主要由<a href="http://buptldy.github.io/2016/10/29/2016-10-29-deconv/" target="_blank" rel="noopener">反卷积层</a>构成, 判别模型就由简单的卷积层组成，最后输出一个判断输入图片是否为真实数据的概率 $P(x)$ 。</p><center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/public/16-11-27/33141448.jpg"><br></center><p>下图为随着迭代次数，DCGAN产生图像的变化过程。</p><center><br><img src="https://openai.com/assets/research/generative-models/learning-gan-ffc4c09e6079283f334b2485ae663a6587d937a45ebc1d8aeac23a67889a3cf5.gif"><br></center><p>训练好网络之后，其中的生成模型和判别模型都有其他的作用。一个训练好的判别模型能够用来对数据提取特征然后进行分类任务。通过输入随机向量生成模型可以产生一些非常有意思的的图片，如下图所示，当输入空间平滑变化时，输出的图片也在平滑转变。</p><center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/public/16-11-25/42718244.jpg"><br></center><p>还有一个非常有意思的属性就是如果对生产模型的输入向量做一些简单的数学运算，那么学习的特征输出也有同样的性质，如下图所示。</p><center><br><img src="https://fb-s-a-a.akamaihd.net/h-ak-xfp1/t39.2365-6/13438466_275356996149902_2140145659_n.jpg"><br></center><h2 id="GAN的训练及其改进"><a href="#GAN的训练及其改进" class="headerlink" title="GAN的训练及其改进"></a>GAN的训练及其改进</h2><p>上面使用GAN产生的图像虽然效果不错，但其实GAN网络的训练过程是非常不稳定的。<br>通常在实际训练GAN中所碰到的一个问题就是判别模型的收敛速度要比生成模型的收敛速度要快很多，通常的做法就是让生成模型多训练几次来赶上生成模型，但是存在的一个问题就是通常生成模型和判别模型的训练是相辅相成的，理想的状态是让生成模型和判别模型在每次的训练过程中同时变得更好。判别模型理想的minimum loss应该为0.5，这样才说明判别模型分不出是真实数据还是生成模型产生的数据。</p><h3 id="Improved-GANs"><a href="#Improved-GANs" class="headerlink" title="Improved GANs"></a>Improved GANs</h3><p><a href="https://arxiv.org/pdf/1606.03498v1.pdf" target="_blank" rel="noopener">Improved techniques for training GANs</a>这篇文章提出了很多改进GANs训练的方法，其中提出一个想法叫<code>Feature matching</code>，之前判别模型只判别输入数据是来自真实数据还是生成模型。现在为判别模型提出了一个新的目标函数来判别生成模型产生图像的统计信息是否和真实数据的相似。让 $f(x)$ 表示判别模型中间层的输出， 新的目标函数被定义为 $|| \mathbb{E}<em>{x \sim p</em>{data}}f(x)  -  \mathbb{E}_{z \sim p_z}f(G(z))||^2_2$, 其实就是要求真实图像和合成图像在判别模型中间层的距离要最小。这样可以防止生成模型在当前判别模型上过拟合。</p><h3 id="InfoGAN"><a href="#InfoGAN" class="headerlink" title="InfoGAN"></a>InfoGAN</h3><p>到这可能有些同学会想到，我要是想通过GAN产生我想要的特定属性的图片改怎么办？普通的GAN输入的是随机的噪声，输出也是与之对应的随机图片，我们并不能控制输出噪声和输出图片的对应关系。这样在训练的过程中也会倒置生成模型倾向于产生更容易欺骗判别模型的某一类特定图片，而不是更好的去学习训练数据的分布，这样对模型的训练肯定是不好的。InfoGAN的提出就是为了解决这一问题，通过对输入噪声添加一些类别信息以及控制图像特征(如mnist数字的角度和厚度)的隐含变量来使得生成模型的输入不在是随机噪声。虽然现在输入不再是随机噪声，但是生成模型可能会忽略这些输入的额外信息还是把输入当成和输出无关的噪声，所以需要定义一个生成模型输入输出的互信息，互信息越高，说明输入输出的关联越大。</p><p>下面三张图片展示了通过分别控制输入噪声的类别信息，数字角度信息，数字笔画厚度信息产生指定输出的图片，可以看出InfoGAN产生图片的效果还是很好的。</p><center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/public/16-11-29/839516.jpg"><br><img src="http://7xritj.com1.z0.glb.clouddn.com/public/16-11-29/10937636.jpg"><br><img src="http://7xritj.com1.z0.glb.clouddn.com/public/16-11-29/2995738.jpg"><br></center><h3 id="其他应用"><a href="#其他应用" class="headerlink" title="其他应用"></a>其他应用</h3><p>GAN网络还有很多其他的有趣应用，比如下图所示的根据<code>一句话来产生对应的图片</code>，可能大家都有了解karpathy大神的<a href="https://github.com/karpathy/neuraltalk2" target="_blank" rel="noopener"><code>看图说话</code></a>, 但是GAN有能力把这个过程给反过来。</p><center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/public/16-11-29/51572272.jpg"><br></center><p>还有下面这个“<a href="https://github.com/bamos/dcgan-completion.tensorflow" target="_blank" rel="noopener">图像补全</a>”, 根据图像剩余的信息来匹配最佳的补全内容。</p><center><br><img src="https://github.com/bamos/dcgan-completion.tensorflow/raw/master/completion.compressed.gif"><br></center><p>还有下面这个<a href="https://swarbrickjones.wordpress.com/2016/01/13/enhancing-images-using-deep-convolutional-generative-adversarial-networks-dcgans/" target="_blank" rel="noopener">图像增强</a>的例子，有点去马赛克的意思，效果还是挺不错的:-D。</p><center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/public/16-11-29/71438836.jpg"><br></center><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>颜乐存说过，2016年深度学习领域最让他兴奋技术莫过于对抗学习。对抗学习确实是解决非监督学习的一个有效方法，而无监督学习一直都是人工智能领域研究者所孜孜追求的“终极目标”之一。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://arxiv.org/abs/1406.2661" target="_blank" rel="noopener">Generative Adversarial Networks</a></p><p><a href="https://arxiv.org/abs/1511.06434" target="_blank" rel="noopener">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</a></p><p><a href="https://arxiv.org/abs/1606.03498" target="_blank" rel="noopener">Improved Techniques for Training GANs</a></p><p><a href="https://arxiv.org/abs/1606.03657" target="_blank" rel="noopener">InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://7xritj.com1.z0.glb.clouddn.com/public/16-11-27/13513902.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
    
    </summary>
    
    
      <category term="Deep Learning" scheme="http://yoursite.com/tags/Deep-Learning/"/>
    
      <category term="Unsupervised Learning" scheme="http://yoursite.com/tags/Unsupervised-Learning/"/>
    
  </entry>
  
</feed>
