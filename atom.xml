<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>muzi-8</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-09-15T05:29:58.761Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>muzi</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>图布局与图链接预测</title>
    <link href="http://yoursite.com/2018/09/14/%E6%96%87%E7%8C%AE%E8%AE%B2%E8%A7%A31/"/>
    <id>http://yoursite.com/2018/09/14/文献讲解1/</id>
    <published>2018-09-14T09:52:52.000Z</published>
    <updated>2018-09-15T05:29:58.761Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="现状"><a href="#现状" class="headerlink" title="现状"></a>现状</h2><p>最近师弟师妹的加入，使得我们小组越来越壮大，然后频繁召开的组会次数越来越多，基本上一周一次，那么对于我们学生而言，压力也是挺大的，需要做大量的调研，讲清楚一篇论文。但是这也是好事，不仅能够让讲得人成长很快，而且也能让听得人“优化一下大脑”。不亦乐乎，故现做一下分享与记录。</p><p>本次主要分享师弟、师妹的两篇文献讲解</p><p>《Revisiting Stress Majorization as a Unified Framework for<br>Interactive Constrained Graph Visualization》</p><p>《weisfeiler-lehman neural machine for link prediction》<br><a id="more"></a></p><h2 id="Part-1"><a href="#Part-1" class="headerlink" title="Part 1"></a>Part 1</h2><p>第一篇文献应该属于图可视化的范畴，关于图可视化这个领域，我们可以查阅A Survey on Graph Visualization<sup>[1]</sup>,也可以直接阅读作者潘嘉铖<sup>[2]</sup>对此综述的理解。</p><p><center><img src="http://od2s78ez3.bkt.clouddn.com/18-9-14/75999373.jpg" alt=""></center><sup>[3]</sup></p><p>此论文就是典型的<strong>旧问题，新方法</strong>。最大创新之处如上图所示，将Strees Majorization的’距离标量’改变为’距离矢量’，然后就有了作者接下来的一系列数学论证，运用到的数学工具是<strong>矩阵</strong>，编程是通过<strong>c++</strong>语言实现的，值得一提的是此论文的实验确实<strong>详实</strong>。</p><p>意义：<br>虽然自己并不是专门研究图可视化领域，但是此论文给我的启发就是顶会上的文章首先写得很<strong>漂亮<sup>[4]</sup></strong>，然后就是作者的<strong>大会视频<sup>[3]</sup></strong>可以借鉴。</p><p>疑惑： D3.js or Three.js的库是怎样引入约束的？</p><h2 id="Part-2"><a href="#Part-2" class="headerlink" title="Part 2"></a>Part 2</h2><p>第二篇文献属于社交网络当中的“边链接（Link Prediction）”预测工作。</p><p>作者提出了一种Wlnm（Weisfeiler-Lehman Neural Machine）方法进行“边链接”预测，对于网络中的每一个边，这种方法首先抽取了一个子图(一个边连接的封闭子图）,紧接着将封闭子图进行图编码转换为连接矩阵形式，然后将连接矩阵输入给神将网络进行一个边链接预测模型的学习。对于此任务的整个流程图如下所示：</p><p><center><img src="http://od2s78ez3.bkt.clouddn.com/18-9-15/21767435.jpg" alt=""></center><br>作者提到自己两大工作就是这个<strong>框架</strong>流程对各种现实中的网络都具有普适性(great generality)、自己提出了一种新的<strong>Graph labeling方法</strong>。</p><ol><li>普适性：由于作者采用的是<strong>神经网络</strong>，自动去学习网路结构中的拓扑特征（topological features），所以相比于之前的手工设计的规则或者经验方法（heuristics methods）。</li><li>Graph label算法：这方面有着很强的图理论知识背景，作者自己提出了一种WL算法的变种，即Palette-WL；</li></ol><p>作者自己实现的流程如如下：</p><p><center><img src="http://od2s78ez3.bkt.clouddn.com/18-9-15/45168166.jpg" alt=""></center><br>在自己实现的过程中首要解决两个问题：1）子图的抽取K的选取。2）运用自己提出的算法进行子图编码。</p><p>疑惑：为什么要进行图编码？图编码如何进行向矩阵转换？</p><h2 id="Part-3"><a href="#Part-3" class="headerlink" title="Part 3"></a>Part 3</h2><p>知识补充：</p><h3 id="图"><a href="#图" class="headerlink" title="图"></a>图</h3><p>图（Graph）表示对象与对象之间关系的方法，两大要素：对象即节点（node）、关系即边（edge）</p><p>一般用两种形式表示图：</p><p>1）G=(V,E);v表示节点的集合，E表示边的集合。</p><p>2）邻接矩阵（Adjacent matrix）：矩阵的第i个节点与第j个节点为1表示有边，为0表示无边。刻画了一个图的结构信息。<br>缺点是：大部分节点没有关系，邻接矩阵特别稀疏，不利于存储计算。</p><h3 id="图嵌入（Network-Embedding）"><a href="#图嵌入（Network-Embedding）" class="headerlink" title="图嵌入（Network Embedding）"></a>图嵌入（Network Embedding）</h3><p>网络表示学习（Network Representation Learning）也称为图嵌入法（Graph Embedding Method）：用<strong>低维、稠密、实值的向量</strong>表示网络中节点。该向量反映了图中的结构，最大的好处就是不再手工提取特征，可以将异质信息投影到同一个低维空间，然后可以输入任何的机器学习模型去解决具体面对的问题。图嵌入的经典应用<strong>推荐</strong>、<strong>节点分类</strong>、<strong>链接预测</strong>、<strong>可视化</strong>。</p><ol><li>在深度学习方法中，我们也经常听到Embedding层。那么深度学习中的Embedding到底是什么含义呢？为什么需要这个层呢<sup>[6]</sup>？</li></ol><p>1） 使用ont-hot编码的向量很高维，也很稀疏</p><p>2） 使用了embeeding之后，在神经网络训练过程中，我们就可以可视化地了解到词语之间的关系。</p><p>面对上述优点与缺点，如何实现这种embedding操作呢？<br>举个栗子：对于一句话如：“deep learning is very deep”经过嵌入层之后可能就会表示为[1,2,3,4,1];原因是我们影射了一种关系，如下所示：</p><p><center><img src="http://od2s78ez3.bkt.clouddn.com/18-9-15/53253924.jpg" alt=""></center><br>我们用留个维度的向量[.32,.02,.48,.21,.56,.15]表示额“deep”这个单词，工程实现上我们会用矩阵的索引“1”来表示单词“deep”<sup>[8]</sup>。</p><h4 id="node-to-vector"><a href="#node-to-vector" class="headerlink" title="node to vector"></a>node to vector</h4><p>像大家熟知的NLP领域中的word to vector<sup>[7]</sup>，大家就在考虑能不能够将这种想法运用到图（Graph）中.基于DeepWalk算法的改进实现<sup>[9]</sup>。形成了自己的启发式方法，然后将一张图（复杂的图结构）转换为一篇“文本”，然后调用现成的word2vec模型来生成向量。</p><p><center><img src="http://od2s78ez3.bkt.clouddn.com/18-9-15/48782617.jpg" alt=""></center></p><h4 id="嵌入为什么能够进行可视化呢？"><a href="#嵌入为什么能够进行可视化呢？" class="headerlink" title="嵌入为什么能够进行可视化呢？"></a>嵌入为什么能够进行可视化呢？</h4><p>node2vector<sup>[9]</sup>作为一种图嵌入（图表示）的方法，因为其遵循了两大原则：homolphily（同质性）；structural equivalence（结构等价性）。那么实际用途可以进行网络的分析。比如对于小说《悲惨世界》人物关系的分析，总共有77个人物，即77个节点。人物之间总共有254个关系，即254个边存在。</p><p><center><img src="http://od2s78ez3.bkt.clouddn.com/18-9-15/27525217.jpg" alt=""></center><br>上图是通过词嵌入可视化之后得到的社区图，下图是通过词嵌入可视化得到<strong>人物角色等价</strong>（有点不好理解）可视化图</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li>Weiwei Cui: A Survey on Graph Visualization</li><li><a href="http://jackieanxis.coding.me/2017/10/03/WeiweiCui%E7%9A%84%E5%9B%BE%E5%8F%AF%E8%A7%86%E5%8C%96%E7%BB%BC%E8%BF%B0%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" target="_blank" rel="noopener">WeiweiCui的图可视化综述阅读笔记</a></li><li><a href="https://vimeo.com/238502264" target="_blank" rel="noopener">论文大会oral video</a></li><li><a href="chrome-extension://ihgdgpjankaehldoaimdlekdidkjfghe/viewer.html#https://homepage.univie.ac.at/michael.sedlmair/papers/wang2017graph.pdf" target="_blank" rel="noopener">Revisiting Stress Majorization as a Unified Framework for<br>Interactive Constrained Graph Visualization</a></li><li><a href="https://zhuanlan.zhihu.com/p/33732033" target="_blank" rel="noopener">Graph Embedding：从DeepWalk到SDNE</a></li><li><a href="https://blog.csdn.net/u010412858/article/details/77848878" target="_blank" rel="noopener">深度学习中Embedding层有什么用？</a></li><li><a href="chrome-extension://ihgdgpjankaehldoaimdlekdidkjfghe/viewer.html#https://arxiv.org/pdf/1301.3781.pdf" target="_blank" rel="noopener">Efficient Estimation ofWord Representations in<br>Vector Space</a></li><li><a href="https://towardsdatascience.com/deep-learning-4-embedding-layers-f9a02d55ac12" target="_blank" rel="noopener">Why You Need to Start Using Embedding Layers?</a></li><li><a href="chrome-extension://ihgdgpjankaehldoaimdlekdidkjfghe/viewer.html#https://cs.stanford.edu/people/jure/pubs/node2vec-kdd16.pdf" target="_blank" rel="noopener">node2vec: Scalable Feature Learning for Networks</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;现状&quot;&gt;&lt;a href=&quot;#现状&quot; class=&quot;headerlink&quot; title=&quot;现状&quot;&gt;&lt;/a&gt;现状&lt;/h2&gt;&lt;p&gt;最近师弟师妹的加入，使得我们小组越来越壮大，然后频繁召开的组会次数越来越多，基本上一周一次，那么对于我们学生而言，压力也是挺大的，需要做大量的调研，讲清楚一篇论文。但是这也是好事，不仅能够让讲得人成长很快，而且也能让听得人“优化一下大脑”。不亦乐乎，故现做一下分享与记录。&lt;/p&gt;
&lt;p&gt;本次主要分享师弟、师妹的两篇文献讲解&lt;/p&gt;
&lt;p&gt;《Revisiting Stress Majorization as a Unified Framework for&lt;br&gt;Interactive Constrained Graph Visualization》&lt;/p&gt;
&lt;p&gt;《weisfeiler-lehman neural machine for link prediction》&lt;br&gt;
    
    </summary>
    
      <category term="文献讲解" scheme="http://yoursite.com/categories/%E6%96%87%E7%8C%AE%E8%AE%B2%E8%A7%A3/"/>
    
    
      <category term="图可视化" scheme="http://yoursite.com/tags/%E5%9B%BE%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>解读 Robust Discriminative Localization Maps</title>
    <link href="http://yoursite.com/2018/09/08/%E6%96%87%E7%8C%AERobustDiscriminative%20Localization%20Maps/"/>
    <id>http://yoursite.com/2018/09/08/文献RobustDiscriminative Localization Maps/</id>
    <published>2018-09-08T07:34:22.000Z</published>
    <updated>2018-09-09T01:53:48.992Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>激活图（Activation maps）已经被用来可视化深度学习模型，并且用来提升网络模型的性能。但是呢，网络模型又特别容易受对抗攻击的影响。虽然复原出分类器原本预测的结果是较难的，经常需要复杂的变换，但是呢，在对抗样本复原出原本的激活图是轻而易举的，既不需要修改分类器，也不需要对输入样本做出变化。<br><a id="more"></a></p><h2 id="激活映射图"><a href="#激活映射图" class="headerlink" title="激活映射图"></a>激活映射图</h2><p>Activation Maps这个概念出自MIT周博磊发表于2016年CVPR上面的一篇论文<sup>[1]</sup>。</p><p>从自然图像中学到的卷积滤波器对于训练集中的1000个类别是敏感的，换句话说就是图像中包含相关类别的区域与卷积核进行卷积操作之后得到较高的激活值，而不包含类别的区域得到较小的激活值。Class Activation Maps(CAM)概念也就在此先验知识下应运而生，并且也验证了此想法。</p><p>Class Activation Maps的应用</p><ul><li><p>物体定位的有效工具</p></li><li><p>提升图像分类网络</p></li><li><p>Visual question answering（VQA）</p></li><li><p>语义图像压缩</p></li><li><p>图像检索</p></li><li><p><strong>注意网路(attention networks)<sup>[2]</sup></strong></p></li></ul><h2 id="对抗的影响"><a href="#对抗的影响" class="headerlink" title="对抗的影响"></a>对抗的影响</h2><p>分类网络已证明容易被“愚弄”<sup>[3]、[4]</sup>，例如白盒攻击（white-box attacks)。 因为对抗的影响，所以根据模型的预测结果即最高概率的类别也就是错误的，对应生成的激活映射图也就是错误的。如下图所示，左侧两张图代表的是由原始图像的预测结果得到的激活映射图，右侧两张图像代表的是由对抗样本图像的网络预测结果得到的激活映射图。可以发现得到的激活图是错的，这是为什么呢？原因就是网络的预测值（最高概率值所属的类别）在起着决定性作用。</p><p><center><img src="http://od2s78ez3.bkt.clouddn.com/18-9-8/21770878.jpg" alt=""></center></p><h2 id="Imagnet数据集的特性"><a href="#Imagnet数据集的特性" class="headerlink" title="Imagnet数据集的特性"></a>Imagnet数据集的特性</h2><p>ImageNet包含1000个类别。这1000个类别中的许多都是细粒度（fine-grained）。在Top-5中预测的类别中，第二个预测的类别很有可能与第一个类别是相近关系(<strong>作者原文提到说也有可能是synonym，但是我认为这是不对的</strong>，依据介绍数据集（ImageNet）的论文<sup>[5]</sup>3.1.1小节提到1000个synsets no overlap each other）。</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>为了使激活映射图对于对抗样本也具有适应性，作者选取了的方法叫做前top-K的映射图指数加权求和（exponentially weighted average of the maps of the top-k classes）。</p><h2 id="结果展示"><a href="#结果展示" class="headerlink" title="结果展示"></a>结果展示</h2><p><center><img src="http://od2s78ez3.bkt.clouddn.com/18-9-8/64573881.jpg" alt=""><br></center><br>最右侧是通过作者提出的方法的到的激活映射图，可以发现基本和最左侧得到的结果一致。</p><p>源码 <a href="https://github.com/iamaaditya/robust-activation-maps" target="_blank" rel="noopener">code</a></p><h2 id="疑惑"><a href="#疑惑" class="headerlink" title="疑惑"></a>疑惑</h2><p>本文最大的困扰点是:作者通过实验证明对抗样本的预测类别与自然图像的预测类别关系介绍。</p><p><center><img src="http://od2s78ez3.bkt.clouddn.com/18-9-8/52753441.jpg" alt=""></center></p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>这篇文章应该是站在别人经典工作的一小步探索，正验证了科研的一种方法。从施引文献中去发现可以改进的工作。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference:"></a>Reference:</h2><ol><li><a href="chrome-extension://ihgdgpjankaehldoaimdlekdidkjfghe/viewer.html#http://cnnlocalization.csail.mit.edu/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf" target="_blank" rel="noopener">Learning deep features for discriminative localization</a></li><li><a href="chrome-extension://ihgdgpjankaehldoaimdlekdidkjfghe/viewer.html#https://arxiv.org/pdf/1802.10171.pdf" target="_blank" rel="noopener">Tell me where to look: Guided attention inference network</a></li><li><a href="https://arxiv.org/abs/1312.6199" target="_blank" rel="noopener">Intriguing properties of neural networks</a></li><li><a href="https://arxiv.org/abs/1412.6572" target="_blank" rel="noopener">Explaining and harnessing adversarial examples</a></li><li>ImageNet Large Scale Visual Recognition Challenge</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;摘要&lt;/h2&gt;&lt;p&gt;激活图（Activation maps）已经被用来可视化深度学习模型，并且用来提升网络模型的性能。但是呢，网络模型又特别容易受对抗攻击的影响。虽然复原出分类器原本预测的结果是较难的，经常需要复杂的变换，但是呢，在对抗样本复原出原本的激活图是轻而易举的，既不需要修改分类器，也不需要对输入样本做出变化。&lt;br&gt;
    
    </summary>
    
      <category term="文献解读" scheme="http://yoursite.com/categories/%E6%96%87%E7%8C%AE%E8%A7%A3%E8%AF%BB/"/>
    
    
      <category term="可解释性AI" scheme="http://yoursite.com/tags/%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7AI/"/>
    
  </entry>
  
  <entry>
    <title>学科发展史</title>
    <link href="http://yoursite.com/2018/08/29/%E5%AD%A6%E7%A7%91%E5%8F%91%E5%B1%95%E5%8F%B2/"/>
    <id>http://yoursite.com/2018/08/29/学科发展史/</id>
    <published>2018-08-28T18:43:31.000Z</published>
    <updated>2018-08-29T04:22:38.704Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="智能科学技术的发展"><a href="#智能科学技术的发展" class="headerlink" title="智能科学技术的发展"></a>智能科学技术的发展</h2><h3 id="起源与发展"><a href="#起源与发展" class="headerlink" title="起源与发展"></a>起源与发展</h3><p>当电子计算机还未出现时，人们就已经开始对人类智能行为进行了初步的探索。二十世纪初期，怀特赫德、罗素、塔斯基和维纳等人在数理逻辑领域做出了巨大贡献，并向机器的逻辑推理迈进一步。此后，丘奇和图灵等人就计算本质问题提出了计算和符号处理的概念，从理论上揭示出形式推理概念和计算机之间的联系。为了测试计算机是具备人类的智能，图灵给出了著名的“图灵测试”。</p><p>1956年夏季，麦卡锡、明斯基、郎彻斯特、香侬与其他学者就机器模拟人类智能问题展开激烈讨论，首次使用了人工智能这一术语。这是智能科学史上具有划时代的意义的第一次人工智能研讨会，标志着人工智能学科的诞生，以及系统、全面的智能科学研究的开始。这些从事数学、心理学、信息论、计算机和神经学的年轻学者，后来都成为著名的人工智能专家，为智能科学的发展做出了巨大的贡献。<br><a id="more"></a><br>近半个世纪来，智能科学在理论研究和应用研究取得了巨大进展，并在发展过程中衍生出众多的分支领域。这些领域包括自然语言处理、自动程序设计、问题求解、逻辑推理与定理证明、专家系统与知识工程、模式识别与机器学习、神经网络、机器人学、进化计算、数据挖掘和知识发现、智能Agent和人工生命等。智能科学已经成为一门以研究智能行为的基本理论和实现技术为目标的多学科交叉的科学，它主要涉及脑科学、认知科学、人工智能、生命科学、数学、心理学、电子学、计算机科学、自动化等学科。可以预计，随着生物和信息理论和技术的深入发展和紧密结合，二十一世纪的智能科学将必定取得重大进展，为推动人类社会和经济的发展做出突出贡献。</p><h3 id="智能科学发展历程"><a href="#智能科学发展历程" class="headerlink" title="智能科学发展历程"></a>智能科学发展历程</h3><ul><li><p>1950年，阿兰.图灵出版《计算机与智能》</p></li><li><p>1956年，约翰.麦卡锡在美国达特茅斯电脑大会上首创“人工智能（AI）”一词。美国卡内基.梅隆大学展示世界上第一个人工智能软件。</p></li><li><p>1958年，约翰.麦卡锡在麻省理工学院发明Lisp语言，一种A.I.语言。</p></li><li><p>1964年，麻省理工学院的丹尼.巴洛向世人展示，电脑能够掌握足够的自然语言从而解决了开发计算机代数词汇程序的难题。</p></li><li><p>1965年，约瑟夫.维岑堡建造了ELIZA，一种互动程序、它能以英语与人就任意话题展开对话。</p></li><li><p>1969年，斯坦福大学研制出集运动、理解和解决问题能力于一身的机器人Shakey。</p></li><li><p>1979年，第一台电脑控制的自动行走器“斯坦福车”诞生。</p></li><li><p>1983年，世界第一家批量生产统一规格电脑的公司“思考机器”诞生。</p></li><li><p>1985年，哈罗德.科岑编写的绘图软件Aaron在A.I.大会亮相。</p></li><li><p>90年代，A.I.技术的发展在各个领域均展示长足发展，学习、教学、案件推理、策划、自然环境认识及方位识别、翻译、乃至游戏软件等领域都瞄准了A.I.的研发。</p></li><li><p>1997年，IBM（国际商用机械公司）制造的电脑“深蓝”击败了国际象棋冠军加里.卡斯帕罗夫。</p></li><li><p>90年代末，以A.I.技术为基础的网络信息搜索软件已是国际互联网的基本构件。</p></li><li><p>2000年，互动机械宠物面世。麻省理工学院推出了会做数十种面部表情的机器人Kisinel。</p></li><li><p>2006年，加拿大多伦多大学教授、神经网络之父——Geoffrey Hinton在顶刊学术刊物《科学》上发表一篇文章，该文章提出了深层网络训练中梯度消失问题的解决方案：<strong>无监督预训练对权值进行初始化+有监督训练微调</strong>。斯坦福大学、纽约大学、加拿大蒙特利尔大学等成为研究深度学习的重镇，至此开启了深度学习在学术界和工业界的浪潮。</p></li><li><p>2012年，Hinton课题组为了证明深度学习的潜力，首次参加ImageNet图像识别比赛，通过构建的CNN网络Alexnet一举多得冠军，且碾压第二名(SVM方法)的分类性能。</p></li></ul><h2 id="计算机技术的发展"><a href="#计算机技术的发展" class="headerlink" title="计算机技术的发展"></a>计算机技术的发展</h2><p>计算机是20世纪人类最伟大的发明之一，它以磅礴之势迅猛发展，用非凡的渗透力和亲和力，融进了每个人的工作、学习和生活之中，已在世界范围内形成了一种新的文化，构造了一种崭新的文明。回顾历史，走向未来，展示计算机的发展与魅力。</p><h3 id="帕斯卡加法器"><a href="#帕斯卡加法器" class="headerlink" title="帕斯卡加法器"></a>帕斯卡加法器</h3><p>17世纪最值得称颂的计算机发明属于法国科学家布莱斯.帕斯卡。帕斯卡16岁在构思一种加法器。为了帮助年迈的父亲计算税款耗费了三年的时间，于19岁完成了被他称为“加法器”的机器。</p><p>帕斯卡加法器是由系列齿轮组成的装置，外壳为用黄铜材料制作的长20英寸、宽4英寸、高3英寸的长方盒子，面板上有一系列显示数字的小窗口，可以进行6位加法和减法运算，并可逢十进一。帕斯卡先后做了50台同样的机器，有的机器计算范围扩大到8位，其中有两台至今还保留在巴黎国立工艺博物馆里。</p><h3 id="第一台电子计算机"><a href="#第一台电子计算机" class="headerlink" title="第一台电子计算机"></a>第一台电子计算机</h3><p>二次世界大战期间的1943年，美国马里兰州阿贝丁试炮场为了试验新式火炮计算弹道参数。莫尔学院以资深教授勃雷纳德为首的30余名科学家承担了研制电子计算机的任务。1946年2月14日，标志人类迈进电脑时代的世界第一台电子计算机ENIAC诞生了。</p><p>ENIAC内部安装了17468只电子管，7200个二极管，70000多只电阻器，10000多只电容器和6000只继电器，电路焊点多达50万个。机器安装在一排2.75米高的金属柜里，占地面积约170平方米，总重量达30吨。ENIAC采用穿孔卡输入输出数据。运算速度达到5000次加法/秒，可以在3毫秒时间内做完两个10位数乘法，用20秒钟算完一条炮弹的轨迹。</p><p>由于ENIAC的程序与计算分离，需几十人用几天的时间接通数百条线路后才可进行几分钟的运算。1945年6月，冯.诺依曼与戈德斯坦、勃克斯等人，提出了基于“存储程序”自动依次执行指令的计算机概念。明确规定计算机包括运算器、逻辑控制装置、存储器、输入和输出设备五大部件，用二进制代替十进制运算。这种“存储程序”体系结构的计算机被统称为“诺依曼机”。</p><p>50年代初，冯.诺依曼主持完成了EDVAC（电子离散变量自动计算机），该机缘于“存储程序”的威力，可用于科学计算及信息检索。整部机器只用了3563只电子管和10000只晶体二极管，以1024个44比特水银延迟线储存程序和数据，消耗电力和占地面积只有ENIAC的三分之一。</p><p>电子管、晶体管、集成电路的发明及微处理器的推出并不断升级，为计算机的发展插上了腾飞的翅膀。</p><p>20世纪90年代，Pentium发布，集成了300多万只晶体管，初期工作在60-66MHz，每秒执行一亿条指令。</p><h2 id="电子信息科学的发展"><a href="#电子信息科学的发展" class="headerlink" title="电子信息科学的发展"></a>电子信息科学的发展</h2><p>1862年，麦克斯韦提出了著名的麦克斯韦方程组。电磁学开始成为科学的理论。1873年，麦克斯韦的《电磁学通论》问世，建立了完整的电磁理论。</p><p>1888年，赫兹发现电磁波。这个发现成了近代科学技术史的一座里程碑，开辟了电子技术的新纪元。</p><p>1895年5月7日在俄国物理化学会的物理学分会上，俄国科学家波波夫成功的演示了无线电收发机的工作，成为一个划时代的壮举。后来苏联政府将这一天定为无线电发明日。</p><p>1896年3月24日波波夫和他的助手在俄国物理化学协会的年会上传送了世界上最早无线电电文。<br>同年6月，21岁的意大利青年马可尼也发明了无线电收发报机，在英国取得了专利。并且在1901年实现了横跨大西洋的无线电通讯。从此无线电通信的距离限制被打破，无线电通信进入一个新时代。1909年底，马可尼获得诺贝尔物理学奖。</p><p>从有线到无线，莫尔斯、法拉第、麦克斯韦、赫兹、波波夫、马可尼这一长串的名字为近代无线电通信事业奠定了坚固的基石。</p><p>1880年，波尔发明了以太阳为光源的光电话。1960年美国人梅曼发明了红宝石激光。1970年美国康奈尔公司成功的制成了传输损耗每千米只有20分贝的光纤。随着波分复用技术的发展，光纤通信迎来了广阔的领域。</p><p>1906年电子三极管的发现使远程无线电通讯成为可能。从电子管、晶体管、集成电路到超大规模集成电路以及计算机的迅速发展，电子信息领域的革命迎来了一波又一波高潮。从无线电收发报机到卫星通讯和光通讯，从收音机到宇宙飞船，电子信息科学的发展将人类文明带进了各个领域。</p><h2 id="微电子技术的发展"><a href="#微电子技术的发展" class="headerlink" title="微电子技术的发展"></a>微电子技术的发展</h2><h3 id="电子管的发明"><a href="#电子管的发明" class="headerlink" title="电子管的发明"></a>电子管的发明</h3><p>1883年，美国科学家爱迪生发现了爱迪生效应——没有连接在电路里的铜丝接收到碳丝发射的热电子产生了微弱的电流。</p><p>1904年，受爱迪生效应启发的弗莱明研制出具有整流、检波功能的“热离子阀”——真空二极管。</p><p>1906年，德.福雷斯特在弗莱明的玻璃管内添加了栅极（栅栏状金属网）。有放大作用的电子三极管由此诞生。</p><p>电子管的发明不仅为通讯、广播、电视等技术的发展铺平了道路，更为计算机的发展奠定了基础。</p><h3 id="晶体管革命"><a href="#晶体管革命" class="headerlink" title="晶体管革命"></a>晶体管革命</h3><p>1947年12月，美国贝尔实验室由肖克莱领导的研究组的成员布拉顿和巴丁用金属和半导体材料制成了一个有放大作用的新器件——晶体管。肖克莱继续研究并于1950年发明了一种“结型晶体管”，成为现代晶体管的始祖。不久，各种型号的晶体管纷纷涌现，不仅能替代电子管整流、检波和放大，而且有体积小、寿命长、发热少、功耗低的优点。1956年，肖克莱、布拉顿和巴丁分享了诺贝尔物理学奖。</p><h3 id="集成电路"><a href="#集成电路" class="headerlink" title="集成电路"></a>集成电路</h3><p>1958年，美国德州仪器公司（TI）的基尔比成功地在不超过4平方毫米的面积上集成了20余个元件。1959年2月6日，基尔比申报了名为“集成电路（IC）”的专利，并于2000年因此获得了诺贝尔物理学奖。同年7月30日，仙童半导体公司采用平面处理技术研制出集成电路也申请到专利。1969年，美国联邦法院就集成电路的发明权裁定这是一项“同时的发明”。</p><p>1961年，德州仪器公司研制出第一台用集成电路组装的计算机。</p><p>20世纪60年代中期，摩尔提出著名的“摩尔定律”。此后集成电路芯片的发展证实了这一预见。电脑随着集成电路的发展上了快车道。</p><h2 id="Reference："><a href="#Reference：" class="headerlink" title="Reference："></a>Reference：</h2><ol><li><a href="https://zhuanlan.zhihu.com/p/29096536" target="_blank" rel="noopener">深度学习(deep learning)发展史</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;智能科学技术的发展&quot;&gt;&lt;a href=&quot;#智能科学技术的发展&quot; class=&quot;headerlink&quot; title=&quot;智能科学技术的发展&quot;&gt;&lt;/a&gt;智能科学技术的发展&lt;/h2&gt;&lt;h3 id=&quot;起源与发展&quot;&gt;&lt;a href=&quot;#起源与发展&quot; class=&quot;headerlink&quot; title=&quot;起源与发展&quot;&gt;&lt;/a&gt;起源与发展&lt;/h3&gt;&lt;p&gt;当电子计算机还未出现时，人们就已经开始对人类智能行为进行了初步的探索。二十世纪初期，怀特赫德、罗素、塔斯基和维纳等人在数理逻辑领域做出了巨大贡献，并向机器的逻辑推理迈进一步。此后，丘奇和图灵等人就计算本质问题提出了计算和符号处理的概念，从理论上揭示出形式推理概念和计算机之间的联系。为了测试计算机是具备人类的智能，图灵给出了著名的“图灵测试”。&lt;/p&gt;
&lt;p&gt;1956年夏季，麦卡锡、明斯基、郎彻斯特、香侬与其他学者就机器模拟人类智能问题展开激烈讨论，首次使用了人工智能这一术语。这是智能科学史上具有划时代的意义的第一次人工智能研讨会，标志着人工智能学科的诞生，以及系统、全面的智能科学研究的开始。这些从事数学、心理学、信息论、计算机和神经学的年轻学者，后来都成为著名的人工智能专家，为智能科学的发展做出了巨大的贡献。&lt;br&gt;
    
    </summary>
    
      <category term="学术历史" scheme="http://yoursite.com/categories/%E5%AD%A6%E6%9C%AF%E5%8E%86%E5%8F%B2/"/>
    
    
      <category term="发展史" scheme="http://yoursite.com/tags/%E5%8F%91%E5%B1%95%E5%8F%B2/"/>
    
  </entry>
  
  <entry>
    <title>node入门Ⅰ</title>
    <link href="http://yoursite.com/2018/08/28/node%E5%85%A5%E9%97%A8%E2%85%A0/"/>
    <id>http://yoursite.com/2018/08/28/node入门Ⅰ/</id>
    <published>2018-08-28T03:10:02.000Z</published>
    <updated>2018-08-28T03:14:52.074Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><center><img src="http://od2s78ez3.bkt.clouddn.com/18-8-28/87076827.jpg" alt=""></center></p><h2 id="题记"><a href="#题记" class="headerlink" title="题记"></a>题记</h2><p>关于node.js的网上教程应该是五花八门,但是找到一个适合自己的”有效”教程又是多么地困难!自己一直喜欢逛github的”All activity”,这里相当于朋友圈的实时动态.<br>最近找到了一款特别适合初学者的教程,即《Node.js包教不包会》的学习仓库,这个教程的优点就是作者的讲解过程特别”接地气”,关注人数达到13.5k,优质教程可见一斑.就这样,再一次开始了技术栈的学习.<br><a id="more"></a></p><h2 id="搭建node-js开发环境"><a href="#搭建node-js开发环境" class="headerlink" title="搭建node.js开发环境"></a>搭建node.js开发环境</h2><h3 id="安装nvm-Node-Version-Manager"><a href="#安装nvm-Node-Version-Manager" class="headerlink" title="安装nvm(Node Version Manager)"></a>安装nvm(Node Version Manager)</h3><p>目的:切换node的各种版本</p><h3 id="安装node"><a href="#安装node" class="headerlink" title="安装node"></a>安装node</h3><p>Node.js是一个基于ChromeV8引擎的JavaScript运行时.通过node官网下载linux环境构建好的二进制文件,解压缩即可,只需注意要修改环境变量.</p><h2 id="最简单的express应用"><a href="#最简单的express应用" class="headerlink" title="最简单的express应用"></a>最简单的express应用</h2><h3 id="包管理器npm"><a href="#包管理器npm" class="headerlink" title="包管理器npm"></a>包管理器npm</h3><p>npm(Node.js Package Manager):自动管理包的依赖,不需要担忧安装包的依赖包问题.安装node的时候即安装了npm.</p><h3 id="框架Express"><a href="#框架Express" class="headerlink" title="框架Express"></a>框架Express</h3><p>Express是一种保持最低程度规模的灵活Node.js web应用程序框架.</p><h3 id="使用外部模块"><a href="#使用外部模块" class="headerlink" title="使用外部模块"></a>使用外部模块</h3><h4 id="命令-npm-install"><a href="#命令-npm-install" class="headerlink" title="命令 npm install"></a>命令 npm install</h4><p>项目部署的时候,我们不必将node_modules目录上传至服务器,只需执行 npm install,则npm会自动读取package.json中的依赖并安装至项目node_modules下面.</p><h4 id="生成package-json文件"><a href="#生成package-json文件" class="headerlink" title="生成package.json文件"></a>生成package.json文件</h4><p>使用命令npm init即可生成一个新的package.json文件,然后使用命令npm install package_name –save,则会在生成的package.json中添加dependencies字段.<br>现在我们可以总结一下如何新建一个项目:</p><ul><li>新建文件夹,使用命令 npm init</li><li>安装依赖 npm install –save PACKAGE_NAME</li><li>写应用逻辑</li></ul><h2 id="异步并发特性"><a href="#异步并发特性" class="headerlink" title="异步并发特性"></a>异步并发特性</h2><p>异步机制与事件编程是node.js的两大特色.Node.js作为现代高性能服务器的代表之一,使用的就是基于I/O多路复用的事件驱动的编程方式.</p><h3 id="手脚架-scaffold"><a href="#手脚架-scaffold" class="headerlink" title="手脚架(scaffold)"></a>手脚架(scaffold)</h3><p>快速搭建一个完整的项目结构,开发者只需要在生成的项目结构的基础上进行开发即可.</p><h3 id="爬虫案例"><a href="#爬虫案例" class="headerlink" title="爬虫案例"></a>爬虫案例</h3><p>介绍两种并发请求方案</p><h4 id="使用eventproxy控制并发"><a href="#使用eventproxy控制并发" class="headerlink" title="使用eventproxy控制并发"></a>使用eventproxy控制并发</h4><h4 id="使用-async-控制并发"><a href="#使用-async-控制并发" class="headerlink" title="使用 async 控制并发"></a>使用 async 控制并发</h4><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>本片文章只是简要的罗列出教程内容的lesson1-lesson5的重点内容,细节问题还有待深究.详细资料可查阅原教程.</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference:"></a>Reference:</h2><ol><li><a href="https://github.com/alsotang/node-lessons" target="_blank" rel="noopener">Node.js包教不包会</a></li><li><a href="https://blog.csdn.net/gupar/article/details/78790699" target="_blank" rel="noopener">ubuntu下载安装最新nodejs二进制文件</a></li><li><a href="http://webfullstack.me/" target="_blank" rel="noopener">Web全栈开发网址导航</a></li><li><a href="https://cnodejs.org/topic/533d6edbc2621e680800e0ea" target="_blank" rel="noopener">深入理解node.js异步编程：基础篇</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;center&gt;&lt;img src=&quot;http://od2s78ez3.bkt.clouddn.com/18-8-28/87076827.jpg&quot; alt=&quot;&quot;&gt;&lt;/center&gt;&lt;/p&gt;
&lt;h2 id=&quot;题记&quot;&gt;&lt;a href=&quot;#题记&quot; class=&quot;headerlink&quot; title=&quot;题记&quot;&gt;&lt;/a&gt;题记&lt;/h2&gt;&lt;p&gt;关于node.js的网上教程应该是五花八门,但是找到一个适合自己的”有效”教程又是多么地困难!自己一直喜欢逛github的”All activity”,这里相当于朋友圈的实时动态.&lt;br&gt;最近找到了一款特别适合初学者的教程,即《Node.js包教不包会》的学习仓库,这个教程的优点就是作者的讲解过程特别”接地气”,关注人数达到13.5k,优质教程可见一斑.就这样,再一次开始了技术栈的学习.&lt;br&gt;
    
    </summary>
    
      <category term="node" scheme="http://yoursite.com/categories/node/"/>
    
    
      <category term="全栈" scheme="http://yoursite.com/tags/%E5%85%A8%E6%A0%88/"/>
    
  </entry>
  
  <entry>
    <title>关于user-study的思考</title>
    <link href="http://yoursite.com/2018/08/23/%E5%85%B3%E4%BA%8Euserstudy%E7%9A%84%E6%80%9D%E8%80%83/"/>
    <id>http://yoursite.com/2018/08/23/关于userstudy的思考/</id>
    <published>2018-08-23T08:45:58.000Z</published>
    <updated>2018-08-23T14:46:39.705Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="题记"><a href="#题记" class="headerlink" title="题记"></a>题记</h2><p>当我2016年9月份进组进行自己的可视化与可视分析的研究的时候，王老师就针对“user study”这个问题进行指导与说明，只是当时太年轻，不太清楚这个概念的重要性，在最近一次组会上面，师弟讲述了“Towards Unambiguous Edge Bundling: Investigating Confluent Drawings for Network Visualization”文章的工作，此论文收录于2017VAST期刊上面，小组讨论再次涉及到UserStudy的概念，现对“用户调研”这一章节工作加以说明。<br><a id="more"></a></p><h2 id="user-study设计"><a href="#user-study设计" class="headerlink" title="user study设计"></a>user study设计</h2><p>这一部分简要分析Confluent Drawing论文中的用户调研这一部分是如何实现的？</p><h3 id="Readability-Study"><a href="#Readability-Study" class="headerlink" title="Readability Study"></a>Readability Study</h3><p>作者对自己的工作提出了一个问题:“Can users correctly perceive links in CDs?”用户能否准确地理解CDs中的边呢？遵循以下问题，作者自己做了一个可控的”用户调研”。</p><ul><li><p>Q1: 并非图理论的专家能否学会读懂CD去完成低可读性任务（low-level readability tasks）？</p></li><li><p>Q2：正如我们所了解到的，传统的边捆绑技术引入了歧义，针对任务（low-level readability tasks）我们能否定量化此错误呢？</p></li><li><p>Q3：这四种技术在性能上是否存在不同之处？</p></li><li><p>Q4：在Power Graph与Bundling Technique中用户的主观参数选择</p></li></ul><h4 id="设计"><a href="#设计" class="headerlink" title="设计"></a>设计</h4><p>作者采用了面向受试者的一个设计(within-subject desigh),即让每一个参与者操作执行如下所示四种方案，并且每种方案都是随机出现在被试者面前。让用户使用这四种技术完成“给出起点与终点找出最短路径”的任务。</p><p><center><img src="http://od2s78ez3.bkt.clouddn.com/18-8-23/55302331.jpg" alt=""></center></p><h4 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h4><h5 id="定量分析"><a href="#定量分析" class="headerlink" title="定量分析"></a>定量分析</h5><p>作者从具体某种任务的正确率、针对不同复杂度的数据、完成的时间3个角度分别衡量了用户对于这4种技术的定量分析，结果如下图所示：</p><p><center><img src="http://od2s78ez3.bkt.clouddn.com/18-8-23/17791912.jpg" alt=""></center></p><h5 id="用户使用主观反馈"><a href="#用户使用主观反馈" class="headerlink" title="用户使用主观反馈"></a>用户使用主观反馈</h5><p>作者让用户对这四种技术在四个方面分别进行0(very bad)-4(very good)打分(打分机制属于李克特量表)，四个方面分别为Overall（整体评价）、Learnability（学习的难度）、Cluter-free（图表达的清晰程度）、Confidence（答案的把握）<sup>[3]</sup>。结果如下：</p><p><center><img src="http://od2s78ez3.bkt.clouddn.com/18-8-23/88641731.jpg" alt=""></center></p><h6 id="李克特量表-Likert-scale"><a href="#李克特量表-Likert-scale" class="headerlink" title="李克特量表(Likert scale)"></a>李克特量表(Likert scale)</h6><p>是一种心理反应量表，常用于问卷中，是目前调查研究使用最广泛的量表。当受测者回答此类问卷的项目时，具体的指出自己对该项陈述的认同程度<sup>[1]</sup>。</p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>粗略的对顶刊文章的用户调研进行了分析。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="https://zh.wikipedia.org/wiki/%E6%9D%8E%E5%85%8B%E7%89%B9%E9%87%8F%E8%A1%A8" target="_blank" rel="noopener">李克特量表</a></li><li><a href="chrome-extension://ihgdgpjankaehldoaimdlekdidkjfghe/viewer.html#http://ialab.it.monash.edu/~dwyer/papers/confluentbundling.pdf" target="_blank" rel="noopener">原文：Towards Unambiguous Edge Bundling: Investigating Confluent Drawings for Network Visualization</a></li><li><a href="http://www.cad.zju.edu.cn/home/vagblog/?p=5185" target="_blank" rel="noopener">浙江大学可视分析小组:调研用Confluent Drawings来构造网络可视化</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;题记&quot;&gt;&lt;a href=&quot;#题记&quot; class=&quot;headerlink&quot; title=&quot;题记&quot;&gt;&lt;/a&gt;题记&lt;/h2&gt;&lt;p&gt;当我2016年9月份进组进行自己的可视化与可视分析的研究的时候，王老师就针对“user study”这个问题进行指导与说明，只是当时太年轻，不太清楚这个概念的重要性，在最近一次组会上面，师弟讲述了“Towards Unambiguous Edge Bundling: Investigating Confluent Drawings for Network Visualization”文章的工作，此论文收录于2017VAST期刊上面，小组讨论再次涉及到UserStudy的概念，现对“用户调研”这一章节工作加以说明。&lt;br&gt;
    
    </summary>
    
      <category term="可视化" scheme="http://yoursite.com/categories/%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
    
      <category term="用户评价" scheme="http://yoursite.com/tags/%E7%94%A8%E6%88%B7%E8%AF%84%E4%BB%B7/"/>
    
  </entry>
  
  <entry>
    <title>文献DeepTracker:Visualizing the Training Process of Convolutional Neural Networks解读Ⅰ</title>
    <link href="http://yoursite.com/2018/08/17/%E6%96%87%E7%8C%AEDeepTrackerVisualizing%20the%20Training%20Process%20of%20Convolutional%20Neural%20Networks%E8%A7%A3%E8%AF%BB%E2%85%A0/"/>
    <id>http://yoursite.com/2018/08/17/文献DeepTrackerVisualizing the Training Process of Convolutional Neural Networks解读Ⅰ/</id>
    <published>2018-08-17T07:58:54.000Z</published>
    <updated>2018-08-20T08:32:57.094Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>这篇文章能够进入到我的视线，缘由也是在听屈老师介绍他们组的工作，这篇文章出自国内香港科技大学屈华民老师组的一篇大作，此文章收录在期刊<a href="https://tist.acm.org/#" target="_blank" rel="noopener">TIST</a>(ACM Transactions on Intelligent Systems and Technology),在自己博士课题方向的刚开始，接触此领域关注的第一篇文章是CNNvis，出自清华大学刘世霞老师组。<br><a id="more"></a></p><h2 id="摘要-动机"><a href="#摘要-动机" class="headerlink" title="摘要/动机"></a>摘要/动机</h2><p>深度卷积网络已经在各个领域取得巨大成功。为了加速训练过程，并且减少试错次数，专家们首先需要理解在训练过程中发生的事情和理解卷积网络的行为，虽然目前有Tensorboard训练可视化平台，但是这个平台只能提供少量的信息，比如训练集/验证集的loss值等。为了跨越此鸿沟，作者提出了一个可视分析系统，DeepTracker，对训练过程的动态信息进行探索和发现隐藏在大量训练日志的异常模式。</p><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>可视隐藏在动态训练过程中的信息对理解CNN网络至关重要（例如：损失值、正确率的变化；权重、梯度、激活值随时间的变化等），不幸地是CNNs网络通常不仅包含了许多相互制约与非线性部分，并且近来网络也变得越宽越深，这些都给专家在推断CNN训练行为带来了很多困难。</p><p>有很多研究工作关注于CNN网络学习到的特征，或者网络训练过程的某些代表性快照(snapshots)。不过，很少有研究关注可视训练的整个动态过程（正是本文献所要进行的工作），目前存在的一些工具，比如：TensorBoard、Nvidia Digist、Deeplearning4j<strong>不能够解决工业级别（industry-level）的训练</strong>（在非常大的训练集Imagenet上训练大型网络），作者也提到现有的这些工作不能说明以下一些问题：</p><ul><li><p>随着迭代次数的增加模型在每一个图像类别的性能变化如何？</p></li><li><p>参数的变化是如何影响每一个类别的分类结果？</p></li><li><p>如此多的层数和如此多的图像类别，哪些才更值得关注？</p></li></ul><p>有了上述这些顾虑，亟需一种<strong>可扩展（scalable）</strong>的可视化解决方案。那么会面临哪些挑战呢？</p><ul><li><p>第一大挑战：处理大量（large-scale）的训练日志数据</p><ul><li>百万参数、上万张验证集、百万迭代次数，然而参数与分类结果需要几个迭代次数就得记录一次</li></ul></li><li><p>第二大挑战：日志信息是异构的</p><ul><li>结构信息（网络结构）、数值信息（神经元权重）、图像信息（验证数据集）、标定数据（分类结果）</li></ul></li></ul><p>面对这两大挑战，作者提出了如下解决方案：</p><p>后端算法：</p><ul><li><p>采用下采样方法存储、预处理、组织数据</p></li><li><p>高效的索引机制保证实时（real-time）交互 </p></li><li><p><strong>面向应用的异常检测方法</strong></p></li><li><p>集成<strong>过滤</strong>、<strong>整合</strong>方法 </p></li></ul><p>前端可视化：</p><ul><li><p>立方体（cube-style）可视化形式揭示神经元权重、验证集数据、训练的迭代次数。从不同的角度进行slice、dice数据。<br>文章的主要贡献点总结如下：</p></li><li><p>系统化分析问题与需求</p></li><li><p>系统化地后端算法与前端可视化方法</p></li><li><p>一些新颖的可视化与交互技巧：hierarchical small multiples、grid-based correlation view、cube-style visualization.</p><h2 id="卷积网络背景知识"><a href="#卷积网络背景知识" class="headerlink" title="卷积网络背景知识"></a>卷积网络背景知识</h2><p><center><img src="http://od2s78ez3.bkt.clouddn.com/18-8-18/97652655.jpg" alt=""></center><br><div align="center">卷积网络示意图</div><br>典型的卷积网络通过一系列的级联层将输入的图像转换成1000维向量，完成图像的分类识别任务。构建一个卷积网络结构主要包括：卷积层、池化层、全连接层。网络的参数一般用高斯分布初始化，参数的优化通过梯度下降方法。</p></li></ul><p>（注：验证集的作用只是用来验证训练过程是否有效，并不会再次更改网络训练后的权重）</p><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><h3 id="卷积网络可视化"><a href="#卷积网络可视化" class="headerlink" title="卷积网络可视化"></a>卷积网络可视化</h3><p>关于CNNs可视化的相关工作，作者将其分为两个类别，以特征为主（feature-oriented）、以演变为主（evolution-oriented）。</p><ul><li>以特征为主的相关工作</li></ul><p>heatmap的形式使得我们可以明确输入图像的哪些部分对分类结果贡献最大，合成图像方法可以使得我们明确针对特定图像的特征与相关神经元之间的关系，寻找使得特定神经元激活值最大的图像块方法使得我们能够明确特定神经元学习到的特定特征。虽然以上工作能够探索特定网络是怎么工作，但是并不能以一个全局的角度去全面审视整个网络的训练情况。</p><ul><li>网络的训练过程与演变</li></ul><p>典型的方法就是挑选出训练过程中几个几个快照片段（snapshots）。这些方法的局限在于选择合适的训练片段进行比较与观察，而对于分析参数的变化过程，一般的都是利用降维方式映射到二维平面进行展现，但是，目前的这些方法受限于可扩展性，即不能拓展到有效分析大型数据集比如：Imagnet；</p><h3 id="时间序列可视化"><a href="#时间序列可视化" class="headerlink" title="时间序列可视化"></a>时间序列可视化</h3><p>（注：时间序列可视化是第一次接触）<br>两种经典的时间序列可视化方法：</p><ul><li><p>一个图空间放置多个线图（place multiple charts in the same graph space to produce overlapping curves）</p></li><li><p>small multiples方法</p></li></ul><h2 id="需求分析"><a href="#需求分析" class="headerlink" title="需求分析"></a>需求分析</h2><p>基于卷积网络训练过程中普遍存在的问题与现象，与3位专家周期探讨与商量，最后该系统确定了6种亟需解决的需求。</p><ul><li><p>普遍存在问题与现象</p><p>超参数（学习率、批次大小、层的数量、每层滤波器数量、权重衰减、动量等）</p><p>训练过程中监测变化情况：loss函数值、训练/验证集错误率、<strong>权重更新速率</strong>、每一层权重、梯度、激活值分布情况。</p><p>经验告诉我们：随着迭代次数的增加loss值与错误率应该减少，这两者值同时增加或者<br>训练集上面的loss值violent fluctuation则暗示了可能存在问题，验证集与训练集错误率明显存在巨大反差，表明可能出现过拟合现象，同时表明网络模型学习能力有限。权重更新速率期望在10-3附近，权重初始化分布应该遵循高斯分布，训练过程中如果出现梯度爆炸或者消失现象都将是一种不好的信号。激活值的分布随着训练过程高层相对于低层越来越稀疏。</p></li></ul><p>这些经验虽然能够从一定程度去指导网络的调试与优化，但是并不能有针对性解决具体训练过程中出现的特定问题，比如训练过程中模型对于不同类别的性能表现情况等</p><ul><li><p>6种需求</p><ul><li><p>R1 神经元权重信息的多面含义</p></li><li><p>R2 不同层的比较</p></li><li><p>R3 跟踪验证集的分类结果</p></li><li><p>R4 检测出重要的迭代阶段</p></li><li><p>R5 不同验证集类别的表现情况</p></li><li><p>R6  相关性探讨</p></li></ul></li></ul><h2 id="系统概览"><a href="#系统概览" class="headerlink" title="系统概览"></a>系统概览</h2><p><img src="http://od2s78ez3.bkt.clouddn.com/18-8-18/84688822.jpg" alt=""></p><div align="center">系统概览图</div><p>DeepTracker是一个基于MEAN.js全栈框架的web应用。后端作者对训练日志进行了层级化组织，制定了特定应用的索引机制进行数据的高效存储便于实时在线分析。对于前端可视化界面，建立了三种协同视图分析，即 Validation view、layer view、correlation view。</p><h2 id="数据收集与分析"><a href="#数据收集与分析" class="headerlink" title="数据收集与分析"></a>数据收集与分析</h2><p>此系统最原始的动机是监控“工业级别”的卷积网络训练过程，所以作者采用的卷积模型是ResNet，使用的数据集ImageNet数据集。面对如此大的一个模型大约需要120个epoch才能达到收敛（batch size=128；iterations=120万次；information volumn = several petabytes；times = 4weeks），作者最终通过与专家商量讨论将信息量控制在ITB之内（采取的措施是：每个epoch内只提取7次迭代变化情况）。</p><p>作者只记录了两种信息类型：卷积层/全连接层神经元的梯度与权重、图像分类结果信息。</p><p>BatchNormalization层的参数信息作者没有去关注，并且也没有关注每一层的激活值形况，并不是说明这两者信息对于网络训练过程不重要，只是不再本文献的讨论范围之内。</p><p>虽然基于前面的考虑，对数据规模进行了缩小，但是仍然存在数据量大的问题，作者应对的措施是建立数据索引机制。包括：</p><ul><li><p>层统计索引：检索每一次迭代任意一层的统计值（均值、方差）</p></li><li><p>层滤波器索引：列出每一次迭代任意一层的滤波器信息</p></li><li><p>迭代滤波索引：搜索任意一次迭代过程中所有层中改变最大的滤波器</p></li><li><p>类别统计索引：抽取所有迭代过程中每一类别的信息</p></li><li><p>类别图像索引：特定类别的特定图像数据信息</p></li></ul><h2 id="可视化分析"><a href="#可视化分析" class="headerlink" title="可视化分析"></a>可视化分析</h2><h3 id="1-Validation-View"><a href="#1-Validation-View" class="headerlink" title="1. Validation View"></a>1. Validation View</h3><h4 id="R3需求"><a href="#R3需求" class="headerlink" title="R3需求"></a>R3需求</h4><p>再进行可视化的设计的时候，需要说明一个很重要的概念就是：可视化代表的含义即可视编码（Visual Encoding）<br>为了解决需求R3,所有类别图像随着网络迭代的变化总情况。作者设计了如下的可视界面<br><img src="http://od2s78ez3.bkt.clouddn.com/18-8-18/81280187.jpg" alt=""></p><p><div align="center">分类不同类别图像的难易程度</div><br>横轴x: 表示网络训练过程的整个迭代次数，并且用红-&gt;绿的颜色渐变表示错误率情况</p><p>纵轴y： 依据不同类别图像的分类趋势变化情况，利用k-mean聚类算法，将1000种类其分为4个簇，每个簇下方的每次迭代错误率是所有簇类的平均值，采用heatmap这种可视形式展现（一种有效的时间序列可视化方法）</p><h4 id="R5需求"><a href="#R5需求" class="headerlink" title="R5需求"></a>R5需求</h4><p>针对具体某一簇的所有类别图像的分类情况。</p><h4 id="R4异常迭代的发现"><a href="#R4异常迭代的发现" class="headerlink" title="R4异常迭代的发现"></a>R4异常迭代的发现</h4><p>作者提出了<strong>一种检测异常迭代的算法</strong>，并且用倒三角与正三角的可视编码形式表示异常与正常情况。</p><h5 id="异常检测"><a href="#异常检测" class="headerlink" title="异常检测"></a>异常检测</h5><p>此异常算法提出的动机来源于：对于一个动态的分类过程，正常的模式应该是假如把某张图像分类正确之后，应该再不会将此分错，而异常的情况在于某些图片分类情况在分类的过程中一直不确定。</p><h3 id="2-Layer-View"><a href="#2-Layer-View" class="headerlink" title="2. Layer View"></a>2. Layer View</h3><h4 id="R1需求、R2需求"><a href="#R1需求、R2需求" class="headerlink" title="R1需求、R2需求"></a>R1需求、R2需求</h4><p>对于这部分的任务设计，作者将网络的结构与层内的统计信息等放置在一起，主要表现的是神经元权重的统计信息。</p><h3 id="3-Correlation-View"><a href="#3-Correlation-View" class="headerlink" title="3. Correlation View"></a>3. Correlation View</h3><p>这一部分主要考虑滤波器与图像之间的模式，对于某些类别的图像可能检测出数个异常迭代情况，针对每个异常迭代伴随着异常滤波器。那么是否在这多个异常迭代中有着相同的滤波器?</p><h4 id="R6需求"><a href="#R6需求" class="headerlink" title="R6需求"></a>R6需求</h4><p>进一步探究网络的参数是如何影响分类结果。<br><img src="http://od2s78ez3.bkt.clouddn.com/18-8-19/49672354.jpg" alt=""></p><p><div align="center">异常层与异常类别之间的异常滤波器个数展示情况</div><br>如上图所示：纵轴表示在训练过程中出现异常迭代的所有异常层个数，横轴表示所有的异常类别，不同方块颜色深度表示异常滤波器的个数。显然这只是一个抽象的表示。</p><h3 id="4-Cube-Visualization"><a href="#4-Cube-Visualization" class="headerlink" title="4. Cube Visualization"></a>4. Cube Visualization</h3><p>主视角：层级别的角度展示；俯视角：验证数据集角度展示；右视角：相关性分析角度展示</p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>本文章只是对“DeepTracker:Visualizing the Training Process of Convolutional Neural Networks”文献的简单转述与理解，后续将继续对此文献进行深入挖掘与分析。</p><p>Reference：</p><ol><li><a href="http://www.cse.ust.hk/~dliuae/" target="_blank" rel="noopener">原文作者：刘冬煜</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章能够进入到我的视线，缘由也是在听屈老师介绍他们组的工作，这篇文章出自国内香港科技大学屈华民老师组的一篇大作，此文章收录在期刊&lt;a href=&quot;https://tist.acm.org/#&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;TIST&lt;/a&gt;(ACM Transactions on Intelligent Systems and Technology),在自己博士课题方向的刚开始，接触此领域关注的第一篇文章是CNNvis，出自清华大学刘世霞老师组。&lt;br&gt;
    
    </summary>
    
      <category term="文献解读" scheme="http://yoursite.com/categories/%E6%96%87%E7%8C%AE%E8%A7%A3%E8%AF%BB/"/>
    
    
      <category term="可解释性AI" scheme="http://yoursite.com/tags/%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7AI/"/>
    
  </entry>
  
  <entry>
    <title>浅谈AdminDemo项目遇到的Debug</title>
    <link href="http://yoursite.com/2018/08/16/%E6%B5%85%E8%B0%88AdminDemo%E9%A1%B9%E7%9B%AE%E9%81%87%E5%88%B0%E7%9A%84Debug/"/>
    <id>http://yoursite.com/2018/08/16/浅谈AdminDemo项目遇到的Debug/</id>
    <published>2018-08-16T12:25:32.000Z</published>
    <updated>2018-08-17T06:01:13.317Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>作为一个工程师，或者一个程序员，可能最讨厌遇到的问题就是：程序出现bug，而作为一个全栈工程师，尤其是前端界面一直不能出现自己想要的效果，此时更是产生焦虑，深深地怀疑“人生”，最糟糕的情况可能会将工程搁置一旁，挫败感增强。</p><p>自己7月末参加了北京大学的可视化暑期学校，在8月上旬借着高温假的时间自己学习了<a href="https://www.zaih.com/mentor/84779110/" target="_blank" rel="noopener">安晓辉老师</a>的<a href="https://edu.csdn.net/course/play/1425/22488" target="_blank" rel="noopener">全栈开发入门课程</a>(MEAN.js)，是一门手把手教学怎么完成一个全后端的工程项目线上课程。每一节过后都有一个实践的实例，最后集成一个大的工程项目:WEB管控系统，此项目开源代码托管于安晓辉老师的github的<a href="https://github.com/foruok/XAdmin" target="_blank" rel="noopener">xdmin</a>项目中。<br><a id="more"></a><br>自己在课程项目当中有两节课就陷入了调试debug过程当中，在实现AdminDemo的作业当中，代码也是跟着视频进行全程敲写的，但是最终运行的前端界面一直出现不了自己的想要结果，查找原因，最终调试查找，记录如下：</p><p>（注：以下两处都是自己在解决bug之后，重现bug，并以记录）</p><h2 id="Debug1：左侧条目一直不能成功显示"><a href="#Debug1：左侧条目一直不能成功显示" class="headerlink" title="Debug1：左侧条目一直不能成功显示"></a>Debug1：左侧条目一直不能成功显示</h2><p><img src="http://od2s78ez3.bkt.clouddn.com/18-8-16/10591553.jpg" alt=""></p><p><div align="center">BUG显示界面</div><br>出现上述情况原因如下：<br><img src="http://od2s78ez3.bkt.clouddn.com/18-8-16/56627235.jpg" alt=""></p><p><div align="center">code工程界面</div><br>代码：</p><pre><code>$scope.menus =[];</code></pre><p>语句的分号使用了<strong>中文分号“；”</strong>，使得前端出现莫名其妙的问题，这个问题之前在使用百度开源的Echarts前端可视化工具也曾遇到过，如此一个小的中英文问题使得前端效果一直出现不了。最终显示界面如下所示：<br><img src="http://od2s78ez3.bkt.clouddn.com/18-8-16/3148652.jpg" alt=""></p><p><div align="center">正常前端界面</div></p><h2 id="Debug2：左侧条目“手风琴折叠样式”不能成功显示"><a href="#Debug2：左侧条目“手风琴折叠样式”不能成功显示" class="headerlink" title="Debug2：左侧条目“手风琴折叠样式”不能成功显示"></a>Debug2：左侧条目“手风琴折叠样式”不能成功显示</h2><p>当使用手风琴折叠样式的前端效果，课程提到需要引入以下几个新的js类库：</p><ul><li><p>angular-1.4.3.min.js</p></li><li><p>angular-animate.js</p></li><li><p>ui-bootstrap-tpls-0.13.3.min.js</p></li></ul><p>正如作者提到下载UI组件的“ui-bootstrap-tpls-0.13.3.min.js”库使用github项目中的gh-pages分支<sup>[1]</sup><br><img src="http://od2s78ez3.bkt.clouddn.com/18-8-16/22700203.jpg" alt=""><sup>[2]</sup></p><p>运行之后前端显示如下：<br><img src="http://od2s78ez3.bkt.clouddn.com/18-8-16/71961703.jpg" alt=""></p><p><div align="center">BUG显示界面</div><br>debug信息如下：<br><img src="http://od2s78ez3.bkt.clouddn.com/18-8-16/24318593.jpg" alt=""></p><p><div align="center">终端运行信息与浏览器调试信息</div><br>运行浏览器调试信息第一行显示如下：<br><img src="http://od2s78ez3.bkt.clouddn.com/18-8-16/68736671.jpg" alt=""></p><p><div align="center">accordion-group.html信息</div><br>由于自己第一次接触ui-bootstrap组件的类库，当时并不清楚出现这个bug的原因所在，然后google此错误，大部分说在自己创建的js中没有使用’ui.bootstrap’,但是自己确实是按照视频当中的语句：</p><pre><code>angular.module(x-admin,[&apos;ui.bootstrap&apos;,&apos;ngAnimate&apos;])</code></pre><p>最后自己的解决方案是将<a href="https://github.com/foruok/XAdmin" target="_blank" rel="noopener">xdmin</a>项目的相关js库替换自己的js库文件，然后再次运行程序，前端界面图下：<br><img src="http://od2s78ez3.bkt.clouddn.com/18-8-16/25667278.jpg" alt=""></p><p><div align="center">正常显示“手风琴折叠样式”界面</div><br>当看到前端界面能够如愿以偿的展示自己的想要结果，顿时感觉生活还是那么得美好！最终找到出现bug原因在于自己下载使用的“ui-bootstrap-tpls-0.13.3.min.js”库有问题：<br><img src="http://od2s78ez3.bkt.clouddn.com/18-8-16/24082741.jpg" alt=""><br>正常的“ui-bootstrap-tpls-0.13.3.min.js”库如下所示：<br><img src="http://od2s78ez3.bkt.clouddn.com/18-8-16/54100499.jpg" alt=""><br>对比可以发现确实是少了一些代码片段，导致前端报错：<strong>Failed to load resource</strong>.当自己再次查找下载源得时候发现原始类库是有相应得代码片段，也不知道自己为什么下载的ui-bootstrap-tpls-0.13.3.min.js竟然少<strong>手风琴折叠样式</strong>代码。</p><h2 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h2><p>作为全栈的一个初学者，上述两个bug错误可能是每一个入门者难免遇到的问题，遂记录下来，若能够对后入坑的同学起到帮助作用，甚是欣慰。深刻体会到前端界面不能如愿以偿的显示自己想要的结果，有可能就是自己的一个很小的错误，然后花费大量的时间去找此问题，过程是多么地令人揪心与烦躁。不妨在遇到bug问题，暂时解决不了可以先放放，比如去跑个步，回来再理清思路，可能就解决了。</p><p>Reference：</p><ol><li><a href="https://www.cnblogs.com/MuYunyun/p/6082359.html" target="_blank" rel="noopener">github的gh-pages分支展示自己的项目</a></li><li><a href="https://blog.csdn.net/foruok/article/details/48173377" target="_blank" rel="noopener">Node.js开发入门-引入UIbootstrap</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作为一个工程师，或者一个程序员，可能最讨厌遇到的问题就是：程序出现bug，而作为一个全栈工程师，尤其是前端界面一直不能出现自己想要的效果，此时更是产生焦虑，深深地怀疑“人生”，最糟糕的情况可能会将工程搁置一旁，挫败感增强。&lt;/p&gt;
&lt;p&gt;自己7月末参加了北京大学的可视化暑期学校，在8月上旬借着高温假的时间自己学习了&lt;a href=&quot;https://www.zaih.com/mentor/84779110/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;安晓辉老师&lt;/a&gt;的&lt;a href=&quot;https://edu.csdn.net/course/play/1425/22488&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;全栈开发入门课程&lt;/a&gt;(MEAN.js)，是一门手把手教学怎么完成一个全后端的工程项目线上课程。每一节过后都有一个实践的实例，最后集成一个大的工程项目:WEB管控系统，此项目开源代码托管于安晓辉老师的github的&lt;a href=&quot;https://github.com/foruok/XAdmin&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;xdmin&lt;/a&gt;项目中。&lt;br&gt;
    
    </summary>
    
      <category term="可视化" scheme="http://yoursite.com/categories/%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
    
      <category term="全栈" scheme="http://yoursite.com/tags/%E5%85%A8%E6%A0%88/"/>
    
  </entry>
  
  <entry>
    <title>CSAPP课程（Computer Systems A Programmer&#39;s Perspective)</title>
    <link href="http://yoursite.com/2018/08/09/CSAPP%E8%AF%BE%E7%A8%8B/"/>
    <id>http://yoursite.com/2018/08/09/CSAPP课程/</id>
    <published>2018-08-09T12:19:46.943Z</published>
    <updated>2018-08-09T12:19:46.953Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
      <category term="Project" scheme="http://yoursite.com/categories/Project/"/>
    
    
      <category term="Computer Systems" scheme="http://yoursite.com/tags/Computer-Systems/"/>
    
  </entry>
  
  <entry>
    <title>机器学习的可解译性</title>
    <link href="http://yoursite.com/2018/07/31/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7/"/>
    <id>http://yoursite.com/2018/07/31/机器学习的可解释性/</id>
    <published>2018-07-31T09:09:12.022Z</published>
    <updated>2017-08-31T01:30:38.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><img src="/2018/07/31/机器学习的可解释性/timg.jpg" title="Machine Learning"><blockquote><p>Interpretation is the process of giving<br>explanations<br>解译性指的是给出可解释的过程</p></blockquote><p>即三个问题</p><ol><li>为什么要给出解译性？</li><li>通过什么途径给出可解释性呢？</li><li>我们又如何去评价这是一种好的可解释方法？</li></ol><h5 id="可解释模型的分类（既不是互相排斥-mutually-exclusive-的分类，也不是文献的罗列-exhaustive-list-）"><a href="#可解释模型的分类（既不是互相排斥-mutually-exclusive-的分类，也不是文献的罗列-exhaustive-list-）" class="headerlink" title="可解释模型的分类（既不是互相排斥(mutually exclusive)的分类，也不是文献的罗列(exhaustive list)）"></a>可解释模型的分类（既不是互相排斥(mutually exclusive)的分类，也不是文献的罗列(exhaustive list)）</h5><p>方法：</p><ol><li><p>搭建模型之前</p><ul><li>数据探索的可视（KMeans KNN 等聚类算法）</li></ul></li><li><p>搭建模型过程当中</p><ul><li>基于规则/基于属性(特征)</li><li>基于实例</li><li>从稀疏角度</li><li>从单调角度（借助制定区间形式定义函数变化的趋势（user specified lattice))</li></ul></li><li><p>给定模型之后(比如深度学习)</p><ul><li>敏感性分析，基于梯度的方法（sensitivity analysis）此方法来自金融风险（自变量与因变量关系）领域</li><li>模型的替换</li><li>隐层、中间层的观察</li></ul><p>评价<br>从不同角度去评价！可参考原文</p><p>Reference：</p><ol><li><a href="https://link.zhihu.com/?target=http://people.csail.mit.edu/beenkim/icml_tutorial.html" target="_blank" rel="noopener">Tutorial on Interpretable Machine Learning by Been Kim</a></li><li><a href="https://www.zhihu.com/question/60374968" target="_blank" rel="noopener">ICML 2017上哪些论文值得关注</a></li></ol></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>解析word文档表格</title>
    <link href="http://yoursite.com/2018/07/31/%E8%A7%A3%E6%9E%90word%E6%96%87%E6%A1%A3/"/>
    <id>http://yoursite.com/2018/07/31/解析word文档/</id>
    <published>2018-07-31T09:09:12.011Z</published>
    <updated>2018-05-14T15:15:10.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><pre><code># -*- coding: utf-8 -*-# from __future__ import print_functionimport sysreload(sys)sys.setdefaultencoding(&apos;utf-8&apos;)import win32comfrom win32com.client import Dispatch, constantsw = win32com.client.Dispatch(&apos;Word.Application&apos;)w.Visible = 1w.Documents.Open( FileName = &apos;D:\muzi\\37.docx&apos;)  #路径不能存在中文doc = w.ActiveDocumentcount = doc.Tables.Count  #统计整个word文档有多少个表a = doc.Tables(1).rows.Count # 返回表格的行数b = doc.Tables(1).columns.Count # 返回表格的列数import collectionsContent = collections.OrderedDict()for num in range (count):    for i in range (doc.Tables(num+1).rows.Count):        key = re.sub(r&apos;\r\x07&apos;,&apos;&apos;,doc.Tables(num+1).Cell(Row= i+1,Column = 1).Range.Text) //正则化去除特殊符号        value = re.sub(r&apos;\r\07&apos;,&apos;&apos;,doc.Tables(num+1).Cell(Row=i+1,Column = 2).Range.Text)         Content[key] = value    filename = &apos;D:\muzi\m&apos;      import json    with open(filename+&apos;.json&apos;,&apos;a&apos;) as outfile:    //打开已存在的文件        json.dump(Content,outfile,ensure_ascii = False) //字典内容写入json文件中        outfile.write(&apos;\n&apos;)</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>opencv离线安装问题</title>
    <link href="http://yoursite.com/2018/07/31/opencv%E5%AE%89%E8%A3%85%E9%97%AE%E9%A2%98/"/>
    <id>http://yoursite.com/2018/07/31/opencv安装问题/</id>
    <published>2018-07-31T09:09:11.991Z</published>
    <updated>2018-04-24T04:11:06.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="step-0"><a href="#step-0" class="headerlink" title="step 0"></a>step 0</h3><p>下载源文件 opencv-2.4.11.zip<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ unzip opencv-2.4.11.zip</span><br></pre></td></tr></table></figure></p><p>在解压路径下面进行编译<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cmake CMakeLists.txt -DWITH_CUDA=ON -DCUDA_ARCH_BIN=<span class="string">"3.2"</span> -DCUDA_ARCH_PTX=<span class="string">""</span> -DBUILD_TESTS=OFF -DBUILD_PERF_TESTS=OFF -DBUILD_NEW_PYTHON_SUPPORT=ON</span><br><span class="line"></span><br><span class="line">make -j4</span><br></pre></td></tr></table></figure></p><a id="more"></a><p>报错0：<br><img src="/2018/07/31/opencv安装问题/161227210742477.png"><br>opencv与cuda8.0不兼容导致，～/opencv/modules/cudalegacy/src/graphcuts.cpp文件内容：<br><img src="/2018/07/31/opencv安装问题/161227210742478.png"><br>OK！重新编译</p><h3 id="step-1"><a href="#step-1" class="headerlink" title="step 1"></a>step 1</h3><p>安装源文件<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ make install</span><br></pre></td></tr></table></figure></p><p>ok 安装成功！<br>opencv默认安装位置为“/usr/local”下lib、bin、include等目录</p><h3 id="step-2"><a href="#step-2" class="headerlink" title="step 2"></a>step 2</h3><p>使用 python import cv2<br>错误1：<br>没有此模块，需要将opencv安装的源文件下面编译生成的共享文件库cv2.so拷贝至使用的python 下面。<br>本人使用的python环境是 /home/muzi-18/anaconda2/lib/python2.7/site-packages<br>错误 2：<br>libstdc++so.6 version ‘GLIBCXX_3.4.21’not found<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ cp /usr/lib/x86_64-linux-gnu/libstdc++.so.6  /home/muzi-18/anaconda2/lib/libstdc++.so.6</span><br></pre></td></tr></table></figure></p><p>ok!<br>python<br> import cv2<br>测试成功！</p><p>Reference：<a href="http://www.linuxidc.com/Linux/2016-12/138870.htm" target="_blank" rel="noopener">http://www.linuxidc.com/Linux/2016-12/138870.htm</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;step-0&quot;&gt;&lt;a href=&quot;#step-0&quot; class=&quot;headerlink&quot; title=&quot;step 0&quot;&gt;&lt;/a&gt;step 0&lt;/h3&gt;&lt;p&gt;下载源文件 opencv-2.4.11.zip&lt;br&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ unzip opencv-2.4.11.zip&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;在解压路径下面进行编译&lt;br&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;cmake CMakeLists.txt -DWITH_CUDA=ON -DCUDA_ARCH_BIN=&lt;span class=&quot;string&quot;&gt;&quot;3.2&quot;&lt;/span&gt; -DCUDA_ARCH_PTX=&lt;span class=&quot;string&quot;&gt;&quot;&quot;&lt;/span&gt; -DBUILD_TESTS=OFF -DBUILD_PERF_TESTS=OFF -DBUILD_NEW_PYTHON_SUPPORT=ON&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;make -j4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Software Install" scheme="http://yoursite.com/categories/Software-Install/"/>
    
    
      <category term="Linux" scheme="http://yoursite.com/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>写给自己的一封信</title>
    <link href="http://yoursite.com/2018/07/31/%E5%86%99%E7%BB%99%E8%87%AA%E5%B7%B1%E7%9A%84%E4%B8%80%E5%B0%81%E4%BF%A1/"/>
    <id>http://yoursite.com/2018/07/31/写给自己的一封信/</id>
    <published>2018-07-31T09:09:11.981Z</published>
    <updated>2018-08-09T13:12:23.511Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote><p>十年饮冰, 难凉热血, 梦还在,黄苍终会眷顾.         –梁启超</p></blockquote><p>北京的周五依旧是行人匆匆,夹杂着兮兮小雨,更是显得匆忙与急促.打着雨伞赶着去实验室,继续完成自己调不完的bug,写不完的程序.真是应了实验室同学的调侃”沐风栉雨搞科研”.<br>惯例周五的实验室必定是空空荡荡,无意在微信朋友圈看到好友江转发了一篇关于NBA比赛报道的文章,若不是他配有文字描述”如果足够努力,最差的结果不就是大器晚成”, 还以为又是司空见惯的NBA球场上那些”飞禽猛兽”不可思议的表现.充满着疑问点开了由”虎扑体育”公众号推送的题为”你生涯最高光的时刻是总决赛MVP吗?而我,就是现在了”.看完文章之后,已然哭成一个傻逼样,心情久久不能平复.<br><a id="more"></a><br>文章讲述了一个主人公名叫”安德烈-英格拉姆”在发展联盟打了整整10年球,度过了长达10年之久的每年领着卑微的2.6W的年薪,同时兼职当家教补贴两个女儿的生活费,终于在自己32岁时迎来了NBA生涯首秀.他在赛后回忆说<strong>“这里的球场看起来更加明亮一些,太出乎意料了,我就是感觉到了一股电流流遍全身,太美妙了热情的观众,耀眼的灯光,这真的是一辈子一次的景象</strong><br>这不就是电影”当幸福来敲门”的克里斯活生生的例子啊!吓得我虎躯一震,生活中真有为梦想而坚持的人? 赶紧打开百度浏览器搜索了关键词<strong>“安德烈-英格拉姆”</strong>,有一篇知乎名为”如何评价「湖人与后卫安德烈-英格拉姆签约至本赛季结束」?”这个鲜活有生命力的运动员形象慢慢在我的脑海中丰富起来,认真看完了网友的所有评论,”大道至简”网友评论道</p><blockquote><p>我很尊重为了梦想在打拼的人们,看到魔术师和沃顿他们轻松的笑说”祝贺”,他激动的只会说”感谢”,我会想到千千万万个他们,像他一样,每天默默努力,终于有朝一日成功的那种激动带点傻呵呵的笑和不知所措,真心尊敬与祝福你,祝贺你的梦想实现,就像你的T恤上的一样”always in pursuit”! ,当你认真踏上球场的那一刻,请相信,全世界都在为你骄傲,加油!<br><img src="http://od2s78ez3.bkt.clouddn.com/18-4-13/81508159.jpg" alt=""></p></blockquote><p>正如毛姆小说”月亮与六便士”中的查尔斯先生,在追逐梦想的路上,他过得穷困潦倒,并且一再遭受命运的摧残,不同的是查尔斯先生到死都没有亲眼看到自己的一幅画能够价值连城,而安德烈-英格拉姆在自己32岁终于踏入了斯台普斯中心(Staples Center).实现了属于自己追逐了十年的梦想.正如书中所说:</p><blockquote><p>难道做自己最想做的事,生活在让你感到舒服的环境里,让你的内心得到安宁是糟践自己吗?难道成为年入上万英镑的外科医生,娶得如花美眷就算是成功吗?</p></blockquote><p>至于自己为什么会哭的一塌糊涂,或许只有相同经历的人才能深深的感受到安德烈的那种无奈与坚毅,对于自己而言,求学之路仍未结束,应该是到了深水区与艰难区,高中因为自己的后知后觉,虽然无心插柳考上了市重点中学,但依旧没能鲤鱼跃龙门,选择了fu读之路,就靠着”黑曼巴”精神顺利步入大学之门,进入大学的第一天目标就非常明确,一定要借着”西北农林科技大学”985这个平台跳出去,完成自己”梦想”,正如我所愿,4年的奋斗换来”中科院电子所”的直博入学书.<br>本科的专业是EE,如今要转向CS,其中的自卑与不知所措,让我一下子失去了方向,没有了曾经的优秀感,还好自己已经行动起来,凭借着自学能力与领悟能力,全方位的去弥补自己知识的不足与能力的欠缺,一切都还在路上,正如 吴军老师在”浪潮之巅”所讲:处在这个”Deep Learning”浪潮之中,要有所作为,不要再次成为观潮者. 不忘初心,砥砺前行!<br>最后以习总书记2018年在亚洲博鳌经济论坛的警句结束啰啰嗦嗦的文章</p><blockquote><p>积土而为山, 积水而为海,幸福和美好的未来不会自己出现,成功属于勇毅而笃行的人</p></blockquote><p>这不正好印证了我的两所大学的校训了嘛,不说了,赶紧去写程序…<br>注:<br>西北农林科技大学 校训 : 诚朴勇毅<br>中国科学院大学 校训: 博学笃志, 格物明德</p><p>Reference:<br><a href="https://www.washingtonpost.com/news/dc-sports-bog/wp/2018/04/10/after-a-decade-in-the-minors-this-32-year-old-finally-got-called-up-by-the-lakers/?noredirect=on&amp;utm_term=.77ed308caff9" target="_blank" rel="noopener">This gray-haired 32-year-old rookie spent 10 years in the minors. His NBA debut was too good to believe</a></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;十年饮冰, 难凉热血, 梦还在,黄苍终会眷顾.         –梁启超&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;北京的周五依旧是行人匆匆,夹杂着兮兮小雨,更是显得匆忙与急促.打着雨伞赶着去实验室,继续完成自己调不完的bug,写不完的程序.真是应了实验室同学的调侃”沐风栉雨搞科研”.&lt;br&gt;惯例周五的实验室必定是空空荡荡,无意在微信朋友圈看到好友江转发了一篇关于NBA比赛报道的文章,若不是他配有文字描述”如果足够努力,最差的结果不就是大器晚成”, 还以为又是司空见惯的NBA球场上那些”飞禽猛兽”不可思议的表现.充满着疑问点开了由”虎扑体育”公众号推送的题为”你生涯最高光的时刻是总决赛MVP吗?而我,就是现在了”.看完文章之后,已然哭成一个傻逼样,心情久久不能平复.&lt;br&gt;
    
    </summary>
    
      <category term="人生" scheme="http://yoursite.com/categories/%E4%BA%BA%E7%94%9F/"/>
    
    
      <category term="思考" scheme="http://yoursite.com/tags/%E6%80%9D%E8%80%83/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://yoursite.com/2018/07/31/Pytorch%E6%95%99%E7%A8%8B(%E4%B8%80)/"/>
    <id>http://yoursite.com/2018/07/31/Pytorch教程(一)/</id>
    <published>2018-07-31T09:09:11.975Z</published>
    <updated>2017-10-18T21:01:52.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><h2 id="PYTORCH-系列教程（一）"><a href="#PYTORCH-系列教程（一）" class="headerlink" title="PYTORCH 系列教程（一）"></a>PYTORCH 系列教程（一）</h2><p>Author：muzi</p><p><title><center>201_torch_numpy</center></title><br><img src="http://od2s78ez3.bkt.clouddn.com/17-10-19/33138087.jpg" alt=""><br><img src="http://od2s78ez3.bkt.clouddn.com/17-10-19/36291573.jpg" alt=""></p><ol><li><p>将numpy数组转换为张量，或者张量转换为数组<br>(convert numpy to tensor or vise versa 反之亦然)</p><p> code<br> import torch<br> import numpy as np<br> np_data = np.arange(6).reshape((2, 3))<br> print np_data<br> torch_data = torch.from_numpy(np_data)<br> print torch_data<br> tensor2array = torch_data.numpy()<br> print tensor2array<br>结果：</p><p> [[0 1 2]<br>  [3 4 5]]</p><p>  0  1  2<br>  3  4  5<br> [torch.LongTensor of size 2x3]</p><p> [[0 1 2]<br>  [3 4 5]]</p></li><li><p>元素取绝对值</p><p> code<br> data = [-1, -2, 1, 2]<br> print data<br> tensor = torch.FloatTensor(data)<br> print tensor<br> print np.abs(data)<br> print torch.abs(tensor)<br>结果：</p><p> [-1, -2, 1, 2]</p><p> -1<br> -2<br>  1<br>  2<br> [torch.FloatTensor of size 4]</p><p> [1 2 1 2]</p><p>  1<br>  2<br>  1<br>  2<br> [torch.FloatTensor of size 4]</p></li><li><p>元素取sin值</p><p> code<br> print np.sin(data)<br> print torch.sin(tensor)<br>结果：</p><p> [-0.84147098 -0.90929743  0.84147098  0.90929743]</p><p> -0.8415<br> -0.9093<br>  0.8415<br>  0.9093<br> [torch.FloatTensor of size 4]</p></li><li><p>张量元素取sigmoid值</p><p> code<br> print tensor.sigmoid()<br>结果</p><p>  0.2689<br>  0.1192<br>  0.7311<br>  0.8808<br> [torch.FloatTensor of size 4]</p></li><li><p>张量元素取指数值</p><p> code<br> print tensor.exp()<br>结果：</p><p> 0.3679<br> 0.1353<br> 2.7183<br> 7.3891<br>  [torch.FloatTensor of size 4]</p></li><li><p>元素取平均值</p><p> code<br> print np.mean(data)<br> print torch.mean(tensor)<br>结果：</p><p> 0.0<br> 0.0<br>7.列表，张量的矩阵运算</p><p> data = [[1,2], [3,4]]<br> print data<br> print type(data)<br> tensor = torch.FloatTensor(data)<br> print np.matmul(data,data)<br> print type(np.matmul(data,data))<br> print torch.mm(tensor,tensor)<br>结果：</p><p> [[1, 2], [3, 4]]<br> <type 'list'=""></type></p><h1 id="数据类型是list，可以通过matmul计算list的矩阵运算。"><a href="#数据类型是list，可以通过matmul计算list的矩阵运算。" class="headerlink" title="数据类型是list，可以通过matmul计算list的矩阵运算。"></a>数据类型是list，可以通过matmul计算list的矩阵运算。</h1><p> [[ 7 10]<br>  [15 22]]<br> <type 'numpy.ndarray'=""><br>   7  10<br>  15  22<br> [torch.FloatTensor of size 2x2]</type></p></li><li><p>数组的矩阵乘法操作；张量的內积运算</p><p> code<br> data = np.array(data)<br> print data<br> print type(data) #数据类型是数组，可以通过dot操作计算矩阵运算<br> print data.dot(data)<br> tensor = torch.FloatTensor(data)<br> print tensor<br> #print tensor.dot(tensor) #不正确的操作，因为维度不匹配<br> data1 = [1,2,3,4]<br> print type(data1)<br> print type(data1)<br> tensor = torch.FloatTensor(data1)<br> print tensor<br> print tensor.dot(tensor) #正确的操作 进行內积运算<br>结果：</p><p> [[1 2]<br>  [3 4]]<br> <type 'numpy.ndarray'=""><br> [[ 7 10]<br>  [15 22]]</type></p><p>  1  2<br>  3  4<br> [torch.FloatTensor of size 2x2]</p><p> [1, 2, 3, 4]</p> <type 'list'=""><p>  1<br>  2<br>  3<br>  4<br> [torch.FloatTensor of size 4]</p><p> 30.0</p></type></li><li><p>张量对应元素相乘</p><p> data =[[1,2],[3,4]]<br> print data<br> print type(data)<br> data = np.array(data)<br> print data        # 数据类型：列表与数组区别<br> print type(data)<br> tensor = torch.FloatTensor(data)<br> print tensor.mm(tensor)<br> print tensor*tensor          # 张量元素的对应乘积<br> data1 = np.arange(1,5)<br> data2 = np.array([1,2,3,4])  # 两种方式 产生(n,)的数组<br> print data1<br> print data2<br> print type(data1)<br> print data1.shape<br> tensor_1D = torch.FloatTensor(data1)<br> tensor_1D_ = torch.from_numpy(data2)   #两种方式 将（n,）的数组转换为 1-D张量<br> print tensor_1D<br> print tensor_1D.dot(tensor_1D)         # 1-D张量类型数据的內积：对应元素相乘求和<br>结果：</p><p> [[1, 2], [3, 4]]<br> <type 'list'=""><br> [[1 2]<br>  [3 4]]<br> <type 'numpy.ndarray'="">    #列表与数组的区别</type></type></p><p>   7  10<br>  15  22<br> [torch.FloatTensor of size 2x2]   #张量矩阵乘法</p></li></ol><pre><code>  1   4  9  16[torch.FloatTensor of size 2x2]  #张量对应元素相乘[1 2 3 4][1 2 3 4]&lt;type &apos;numpy.ndarray&apos;&gt;(4,) 1 2 3 4[torch.FloatTensor of size 4]&lt;class &apos;torch.FloatTensor&apos;&gt;30.0</code></pre><p>Reference :<br><a href="https://github.com/MorvanZhou/PyTorch-Tutorial" target="_blank" rel="noopener">莫烦：pytorch教程</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://yoursite.com/2018/07/31/hello-world/"/>
    <id>http://yoursite.com/2018/07/31/hello-world/</id>
    <published>2018-07-31T08:25:38.829Z</published>
    <updated>2018-07-31T08:25:38.829Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Generative Adversarial Networks</title>
    <link href="http://yoursite.com/2016/11/27/2016-11-27-gans/"/>
    <id>http://yoursite.com/2016/11/27/2016-11-27-gans/</id>
    <published>2016-11-27T02:00:00.000Z</published>
    <updated>2018-04-24T18:09:04.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="http://7xritj.com1.z0.glb.clouddn.com/public/16-11-27/13513902.jpg" alt=""><br><a id="more"></a></p><p>人工智能目前的核心目标应该是赋予机器自主理解我们所在世界的能力。对于人类来说，我们对这个世界所了解的知识可能很快就会忘记，比如我们所处的三维环境中，物体能够交互，移动，碰撞；什么动物会飞，什么动物吃草等等。这些巨大的并且不断扩大的信息现在是很容易被机器获取的，问题的关键是怎么设计模型和算法让机器更好的去分析和理解这些数据中所蕴含的宝藏。</p><p><code>Generative models</code>(生成模型)现在被认为是能够实现这一目标的最有前景的方法之一。<code>Generative models</code>通过输入一大堆特定领域的数据进行训练（比如图像，句子，声音等）来使得模型能够产生和输入数据相似的输出。这一直觉的背后可以由下面名言阐述。</p><blockquote><p>“What I cannot create, I do not understand.”  —Richard Feynman</p></blockquote><p>生成模型由一个参数数量比训练数据少的多神经网络构成，所以生成模型为了能够产生和训练数据相似的输出就会迫使自己去发现数据中内在的本质内容。训练<code>Generative models</code>的方法有几种，在这里我们主要阐述其中的<code>Adversarial Training</code>（对抗训练）方法。</p><h2 id="Adversarial-Training"><a href="#Adversarial-Training" class="headerlink" title="Adversarial Training"></a>Adversarial Training</h2><p>上文说过Adversarial Training是训练生成模型的一种方法。为了训练生成模型，Adversarial Training提出一种<code>Discriminative Model</code>(判别模型)来和生成模型产生对抗，下面来说说<code>Generative models</code> $G(z)$ 和 <code>Discriminative Model</code> $D(x)$ 是如何相互作用的。</p><ul><li>生成模型的目标是模仿输入训练数据, 通过输入一个随机噪声来产生和训练数据相似的样本；</li><li>判别模型的目标就是判断生成模型产生的样本和真实的输入样本之间的相似性。</li></ul><p>其中生成模型和判别模型合起来的框架被称为<code>GAN</code>网络。通过下图我们来理清判别模型和生成模型之间的输入输出关系：生成模型通过输入随机噪声 $z(z \sim p_z)$ 产生合成样本；而判别模型通过分别输入真实的训练数据和生成模型的训练数据来判断输入的数据是否真实。</p><center><br><img src="https://culurciello.github.io/assets/unsup/gan_simple.svg" alt=""><br></center><p>描述了<code>GAN</code>的网络结构，但它的优化目标是什么？怎么就可以通过训练使得生成模型能够产生和真实数据相似的输出？优化的目标其实很简单，简单来说就是：</p><ul><li>判别模型努力的想把真实的数据预测为<code>1</code>，把生成的数据预测为<code>0</code>；</li><li>而生成模型的奋斗目标则为‘我’要尽力的让判别模型对‘我’生成的数据预测为<code>1</code>，让判别模型分不清‘我’产生的数据和真实数据之间的区别，从而达到‘以假乱真’的效果。</li></ul><p>下面用形式化说明下如果训练GAN网络, 先定义一些参数：</p><table><thead><tr><th style="text-align:center">参数</th><th style="text-align:center">含义</th></tr></thead><tbody><tr><td style="text-align:center">$p_z$</td><td style="text-align:center">输入随机噪声 $z$ 的分布</td></tr><tr><td style="text-align:center">$p_{data}$</td><td style="text-align:center">未知的输入样本的数据分布</td></tr><tr><td style="text-align:center">$p_g$</td><td style="text-align:center">生成模型的输出样本的数据分布，GAN的目标就是要$p_g=p_{data}$</td></tr></tbody></table><p>训练判别模型 $D(x)$ 的目标：</p><ol><li>对每一个输入数据 $x \sim p_{data}$ 要使得 $D(x)$ 最大；</li><li>对每一个输入数据 $x \nsim p_{data}$ 要使得 $D(x)$ 最小。</li></ol><p>训练生成模型 $G(z)$ 的目标是来产生样本来欺骗判别模型 $D$, 因此目标为最大化 $D(G(z))$，也就是把生成模型的输出输入到判别模型，然后要让判别模型预测其为真实数据。同时，最大化 $D(G(z))$ 等同于最小化 $1-D(G(z))$，因为 $D$ 的输出是介于0到1之间的，真实数据努力预测为1，否则为0。</p><p>所以把生成模型和判别模型的训练目标结合起来，就得到了<code>GAN</code>的优化目标：</p><p>$$\min_G \max_D {\mathbb E}<em>{x\sim p</em>{\rm data}} \log D(x)+{\mathbb E}_{z\sim p_z}[\log (1-D(G(z)))] $$</p><p>总结一下上面的内容，GAN启发自博弈论中的二人零和博弈，在二人零和博弈中，两位博弈方的利益之和为零或一个常数，即一方有所得，另一方必有所失。GAN模型中的两位博弈方分别由生成模型和判别模型充当。生成模型G捕捉样本数据的分布，判别模型是一个二分类器，估计一个样本来自于训练数据（而非生成数据）的概率。G和D一般都是非线性映射函数，例如多层感知机、卷积神经网络等。生成模型的输入是一些服从某一简单分布（例如高斯分布）的随机噪声z，输出是与训练图像相同尺寸的生成图像。向判别模型D输入生成样本，对于D来说期望输出低概率（判断为生成样本），对于生成模型G来说要尽量欺骗D，使判别模型输出高概率（误判为真实样本），从而形成竞争与对抗。</p><h2 id="GAN实现"><a href="#GAN实现" class="headerlink" title="GAN实现"></a>GAN实现</h2><p>一个简单的一维数据GAN网络的tensorflow实现:<a href="https://github.com/ericjang/genadv_tutorial" target="_blank" rel="noopener">genadv_tutorial</a><br>其一维训练数据分布如下所示，是一个均值-1， $\sigma =1$ 的正态分布。</p><center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/public/16-11-24/4360345.jpg" alt=""><br></center><p>我们结合代码和上面的理论内容来分析下GAN的具体实现，判别模型的优化目标为最大化下式，其中 $D_1(x)$ 表示判别真实数据, $D_2(G(z))$ 表示对生成的数据进行判别， 其中 $D_1$ 和 $D_2$ 是共享参数的， 也就是说是同一个判别模型。</p><p>$$\log(D_1(x))+\log(1-D_2(G(z)))$$</p><p>对应的python代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">batch=tf.Variable(<span class="number">0</span>)</span><br><span class="line">obj_d=tf.reduce_mean(tf.log(D1)+tf.log(<span class="number">1</span>-D2))</span><br><span class="line">opt_d=tf.train.GradientDescentOptimizer(<span class="number">0.01</span>)</span><br><span class="line">              .minimize(<span class="number">1</span>-obj_d,global_step=batch,var_list=theta_d)</span><br></pre></td></tr></table></figure></p><p>为了优化 $G$, 我们想要最大化 $D_2(x’)$(成功欺骗 $D$ )，因此 $G$ 的优化函数为：</p><p>$$\log(D_2(G(z)))$$</p><p>对应的python代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">batch=tf.Variable(<span class="number">0</span>)</span><br><span class="line">obj_g=tf.reduce_mean(tf.log(D2))</span><br><span class="line">opt_g=tf.train.GradientDescentOptimizer(<span class="number">0.01</span>)</span><br><span class="line">              .minimize(<span class="number">1</span>-obj_g,global_step=batch,var_list=theta_g)</span><br></pre></td></tr></table></figure><p>定义好优化目标后，下面就是训练的主要代码了：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Algorithm 1, GoodFellow et al. 2014</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(TRAIN_ITERS):</span><br><span class="line">    x= np.random.normal(mu,sigma,M) <span class="comment"># sample minibatch from p_data</span></span><br><span class="line">    z= np.random.random(M)  <span class="comment"># sample minibatch from noise prior</span></span><br><span class="line">    sess.run(opt_d, &#123;x_node: x, z_node: z&#125;) <span class="comment"># update discriminator D</span></span><br><span class="line">    z= np.random.random(M) <span class="comment"># sample noise prior</span></span><br><span class="line">    sess.run(opt_g, &#123;z_node: z&#125;) <span class="comment"># update generator G</span></span><br></pre></td></tr></table></figure></p><p>下面是实验的结果，左图是训练之间的数据，可以看到生成数据的分布和训练数据相差甚远；右图是训练后的数据分析，生成数据和训练数据的分布接近了很多，且此时判别模型的输出分布在0.5左右，说明生成模型顺利的欺骗到判别模型。</p><figure><br>    <img src="http://7xritj.com1.z0.glb.clouddn.com/public/16-11-29/64984825.jpg"><br><br></figure><h2 id="DCGAN"><a href="#DCGAN" class="headerlink" title="DCGAN"></a>DCGAN</h2><p>GAN的一个改进模型就是DCGAN。这个网络的生成模型的输入为一个100个符合均匀分布的随机数（通常被称为<code>code</code>），然后产生输出为64x64x3的输出图像(下图中 $G(z)$ ), 当<code>code</code>逐渐递增时，生成模型输出的图像也逐渐变化。下图中的生产模型主要由<a href="http://buptldy.github.io/2016/10/29/2016-10-29-deconv/" target="_blank" rel="noopener">反卷积层</a>构成, 判别模型就由简单的卷积层组成，最后输出一个判断输入图片是否为真实数据的概率 $P(x)$ 。</p><center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/public/16-11-27/33141448.jpg"><br></center><p>下图为随着迭代次数，DCGAN产生图像的变化过程。</p><center><br><img src="https://openai.com/assets/research/generative-models/learning-gan-ffc4c09e6079283f334b2485ae663a6587d937a45ebc1d8aeac23a67889a3cf5.gif"><br></center><p>训练好网络之后，其中的生成模型和判别模型都有其他的作用。一个训练好的判别模型能够用来对数据提取特征然后进行分类任务。通过输入随机向量生成模型可以产生一些非常有意思的的图片，如下图所示，当输入空间平滑变化时，输出的图片也在平滑转变。</p><center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/public/16-11-25/42718244.jpg"><br></center><p>还有一个非常有意思的属性就是如果对生产模型的输入向量做一些简单的数学运算，那么学习的特征输出也有同样的性质，如下图所示。</p><center><br><img src="https://fb-s-a-a.akamaihd.net/h-ak-xfp1/t39.2365-6/13438466_275356996149902_2140145659_n.jpg"><br></center><h2 id="GAN的训练及其改进"><a href="#GAN的训练及其改进" class="headerlink" title="GAN的训练及其改进"></a>GAN的训练及其改进</h2><p>上面使用GAN产生的图像虽然效果不错，但其实GAN网络的训练过程是非常不稳定的。<br>通常在实际训练GAN中所碰到的一个问题就是判别模型的收敛速度要比生成模型的收敛速度要快很多，通常的做法就是让生成模型多训练几次来赶上生成模型，但是存在的一个问题就是通常生成模型和判别模型的训练是相辅相成的，理想的状态是让生成模型和判别模型在每次的训练过程中同时变得更好。判别模型理想的minimum loss应该为0.5，这样才说明判别模型分不出是真实数据还是生成模型产生的数据。</p><h3 id="Improved-GANs"><a href="#Improved-GANs" class="headerlink" title="Improved GANs"></a>Improved GANs</h3><p><a href="https://arxiv.org/pdf/1606.03498v1.pdf" target="_blank" rel="noopener">Improved techniques for training GANs</a>这篇文章提出了很多改进GANs训练的方法，其中提出一个想法叫<code>Feature matching</code>，之前判别模型只判别输入数据是来自真实数据还是生成模型。现在为判别模型提出了一个新的目标函数来判别生成模型产生图像的统计信息是否和真实数据的相似。让 $f(x)$ 表示判别模型中间层的输出， 新的目标函数被定义为 $|| \mathbb{E}<em>{x \sim p</em>{data}}f(x)  -  \mathbb{E}_{z \sim p_z}f(G(z))||^2_2$, 其实就是要求真实图像和合成图像在判别模型中间层的距离要最小。这样可以防止生成模型在当前判别模型上过拟合。</p><h3 id="InfoGAN"><a href="#InfoGAN" class="headerlink" title="InfoGAN"></a>InfoGAN</h3><p>到这可能有些同学会想到，我要是想通过GAN产生我想要的特定属性的图片改怎么办？普通的GAN输入的是随机的噪声，输出也是与之对应的随机图片，我们并不能控制输出噪声和输出图片的对应关系。这样在训练的过程中也会倒置生成模型倾向于产生更容易欺骗判别模型的某一类特定图片，而不是更好的去学习训练数据的分布，这样对模型的训练肯定是不好的。InfoGAN的提出就是为了解决这一问题，通过对输入噪声添加一些类别信息以及控制图像特征(如mnist数字的角度和厚度)的隐含变量来使得生成模型的输入不在是随机噪声。虽然现在输入不再是随机噪声，但是生成模型可能会忽略这些输入的额外信息还是把输入当成和输出无关的噪声，所以需要定义一个生成模型输入输出的互信息，互信息越高，说明输入输出的关联越大。</p><p>下面三张图片展示了通过分别控制输入噪声的类别信息，数字角度信息，数字笔画厚度信息产生指定输出的图片，可以看出InfoGAN产生图片的效果还是很好的。</p><center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/public/16-11-29/839516.jpg"><br><img src="http://7xritj.com1.z0.glb.clouddn.com/public/16-11-29/10937636.jpg"><br><img src="http://7xritj.com1.z0.glb.clouddn.com/public/16-11-29/2995738.jpg"><br></center><h3 id="其他应用"><a href="#其他应用" class="headerlink" title="其他应用"></a>其他应用</h3><p>GAN网络还有很多其他的有趣应用，比如下图所示的根据<code>一句话来产生对应的图片</code>，可能大家都有了解karpathy大神的<a href="https://github.com/karpathy/neuraltalk2" target="_blank" rel="noopener"><code>看图说话</code></a>, 但是GAN有能力把这个过程给反过来。</p><center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/public/16-11-29/51572272.jpg"><br></center><p>还有下面这个“<a href="https://github.com/bamos/dcgan-completion.tensorflow" target="_blank" rel="noopener">图像补全</a>”, 根据图像剩余的信息来匹配最佳的补全内容。</p><center><br><img src="https://github.com/bamos/dcgan-completion.tensorflow/raw/master/completion.compressed.gif"><br></center><p>还有下面这个<a href="https://swarbrickjones.wordpress.com/2016/01/13/enhancing-images-using-deep-convolutional-generative-adversarial-networks-dcgans/" target="_blank" rel="noopener">图像增强</a>的例子，有点去马赛克的意思，效果还是挺不错的:-D。</p><center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/public/16-11-29/71438836.jpg"><br></center><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>颜乐存说过，2016年深度学习领域最让他兴奋技术莫过于对抗学习。对抗学习确实是解决非监督学习的一个有效方法，而无监督学习一直都是人工智能领域研究者所孜孜追求的“终极目标”之一。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://arxiv.org/abs/1406.2661" target="_blank" rel="noopener">Generative Adversarial Networks</a></p><p><a href="https://arxiv.org/abs/1511.06434" target="_blank" rel="noopener">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</a></p><p><a href="https://arxiv.org/abs/1606.03498" target="_blank" rel="noopener">Improved Techniques for Training GANs</a></p><p><a href="https://arxiv.org/abs/1606.03657" target="_blank" rel="noopener">InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://7xritj.com1.z0.glb.clouddn.com/public/16-11-27/13513902.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
    
    </summary>
    
    
      <category term="Deep Learning" scheme="http://yoursite.com/tags/Deep-Learning/"/>
    
      <category term="Unsupervised Learning" scheme="http://yoursite.com/tags/Unsupervised-Learning/"/>
    
  </entry>
  
</feed>
